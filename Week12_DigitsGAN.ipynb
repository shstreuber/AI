{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/AI/blob/main/Week12_DigitsGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generative Adversarial Network**\n",
        "\n",
        "Taken from Datacamp tutorial: https://www.datacamp.com/tutorial/generative-adversarial-networks\n",
        "\n",
        "Explanations by Professor Sonja Streuber"
      ],
      "metadata": {
        "id": "3upkQ6Ewh3sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**0. Import Libraries and Set Up Environment**\n",
        "\n",
        "The code below imports libraries needed to build and train a neural network using Keras.\n",
        "\n",
        "* `os, numpy, matplotlib.pyplot`, and `tqdm` help with file handling, data manipulation, visualization, and displaying progress bars.\n",
        "\n",
        "* `Input, Dense, Dropout,` and `LeakyReLU` are used to define layers in the neural network.\n",
        "\n",
        "* `mnist` is a dataset of handwritten digits used for training and testing the model.\n",
        "\n",
        "* `Adam` is an optimizer used to improve model performance during training.\n",
        "\n",
        "In summary, this code prepares for building a neural network to classify handwritten digits from the MNIST dataset."
      ],
      "metadata": {
        "id": "DhtMj6TwTvcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znLi60rTS18A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from keras.layers import Input\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import Dense, Dropout\n",
        "#from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**0.1 Set Up Reproducibility and Load Dataset**"
      ],
      "metadata": {
        "id": "d_83Mgcsh2KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To make sure that we can reproduce the experiment and get the same results\n",
        "# Use last 2 digits of your student ID for the seed value.\n",
        "np.random.seed(10)\n",
        "\n",
        "# The dimension of our random noise vector.\n",
        "random_dim = 100"
      ],
      "metadata": {
        "id": "FO0b7SLBTA7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_minst_data():\n",
        "    # load the data\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # normalize our inputs to be in the range[-1, 1]\n",
        "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
        "    # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n",
        "    # 784 columns per row\n",
        "    x_train = x_train.reshape(60000, 784)\n",
        "    return (x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "cL8NxA_eTsZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the block above, the `load_mnist_data` function loads the MNIST dataset, which consists of handwritten digits, using Keras' `mnist.load_data()`. It then normalizes the training images (`x_train`) to have values in the range of [-1, 1] by subtracting 127.5 and dividing by 127.5. The function reshapes the input data from a 3D array of shape (60000, 28, 28) into a 2D array of shape (60000, 784), where each 28x28 image is flattened into a 784-dimensional vector. Finally, the function returns the normalized and reshaped training data, along with the corresponding labels (`y_train`), as well as the test data and labels (`x_test`, `y_test`)."
      ],
      "metadata": {
        "id": "uG4l4cx_UQu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Build Generative Adversarial Network**\n",
        "Now, we are going to define the functions to create the optimizer, generator, and discriminator models used in a Generative Adversarial Network (GAN):"
      ],
      "metadata": {
        "id": "-aXcnA9tUkou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.1 Define Optimizer**\n",
        "The `get_optimizer` function defines and returns an Adam optimizer for training."
      ],
      "metadata": {
        "id": "cYShsgQnVfmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer():\n",
        "    return Adam(lr=0.0002, beta_1=0.5)"
      ],
      "metadata": {
        "id": "nbh-MiqmVTyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `Adam`: The Adam optimizer is an adaptive learning rate optimizer that adjusts the learning rate for each parameter, which makes it efficient and effective for large datasets and models.\n",
        "\n",
        "* `lr=0.0002`: The learning rate is set to 0.0002, which controls how much the model weights should be adjusted during training.\n",
        "\n",
        "* `beta_1=0.5`: This parameter is the exponential decay rate for the first moment estimate (the moving average of the gradient). It is set to 0.5 to make the optimizer more responsive to updates during training.\n",
        "\n",
        "The optimizer is returned to be used when compiling models.\n",
        "\n"
      ],
      "metadata": {
        "id": "SshQVY9WVU-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.2 Build the Generator**\n",
        "\n",
        "Next, we will build the `get_generator` function."
      ],
      "metadata": {
        "id": "S9m0svdWYgJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_generator(optimizer):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(256, input_dim=random_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Dense(1024))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Dense(784, activation='tanh'))\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return generator"
      ],
      "metadata": {
        "id": "N8IxIzBJV0r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function builds and returns a generator model, which is part of a Generative Adversarial Network (GAN).\n",
        "\n",
        "**Explanation**:\n",
        "\n",
        "* `Sequential()`: This indicates that the model is built layer by layer in a linear stack.\n",
        "\n",
        "* `Dense(256, input_dim=random_dim)`: The first layer is a dense (fully connected) layer with 256 units. The input dimension is random_dim, which typically represents the size of the random noise vector used to generate data.\n",
        "\n",
        "* `kernel_initializer=initializers.RandomNormal(stddev=0.02)`: This initializes the weights of the layer with a normal distribution and a standard deviation of 0.02, which helps in breaking symmetry and improving learning.\n",
        "\n",
        "* `LeakyReLU(0.2)`: This is an activation function that introduces non-linearity. It uses a small slope (0.2) for negative values, avoiding the \"dying ReLU\" problem.\n",
        "\n",
        "The model continues adding dense layers of sizes 512 and 1024, each followed by LeakyReLU activation.\n",
        "\n",
        "* `Dense(784, activation='tanh')`: The final output layer generates 784 outputs, corresponding to a 28x28 image (784 pixels). The tanh activation ensures the outputs are scaled between -1 and 1.\n",
        "\n",
        "* `generator.compile(loss='binary_crossentropy', optimizer=optimizer)`: The generator is compiled with a binary cross-entropy loss function (used for binary classification) and the Adam optimizer passed as an argument.\n",
        "\n",
        "Finally, the generator model is returned."
      ],
      "metadata": {
        "id": "0jo9E-FPWE9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.3 Build the Discriminator**\n",
        "\n",
        "Lastly, we need the `get_discriminator` function."
      ],
      "metadata": {
        "id": "yCCoaM19Ys83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discriminator(optimizer):\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "\n",
        "    discriminator.add(Dense(512))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return discriminator"
      ],
      "metadata": {
        "id": "LN7qyqVXWqYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `get_discriminator` function builds and returns a discriminator model, another key component of a Generative Adversarial Network (GAN).\n",
        "\n",
        "**Explanation**:\n",
        "\n",
        "* `Sequential()`: The discriminator model is built layer by layer.\n",
        "\n",
        "* `Dense(1024, input_dim=784)`: The first layer is a dense layer with 1024 units and an input dimension of 784 (each input corresponds to a flattened 28x28 image).\n",
        "\n",
        "* `LeakyReLU(0.2)`: As in the generator, this activation function is used to add non-linearity with a small slope (0.2) for negative values.\n",
        "\n",
        "* `Dropout(0.3)`: Dropout is applied to each layer to prevent overfitting. It randomly drops 30% of the connections during training.\n",
        "\n",
        "The model continues with two more dense layers of sizes 512 and 256, each followed by LeakyReLU activation and Dropout.\n",
        "\n",
        "* `Dense(1, activation='sigmoid')`: The output layer has a single unit with a sigmoid activation function. This layer outputs a probability (between 0 and 1) indicating whether the input image is real (1) or fake (0).\n",
        "\n",
        "* `discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)`: The discriminator is compiled with a binary cross-entropy loss function (for binary classification) and the Adam optimizer passed as an argument.\n",
        "\n",
        "Finally, the discriminator model is returned."
      ],
      "metadata": {
        "id": "dVW41AVuWw13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4 Assemble and Compile the GAN**\n",
        "\n",
        "The following `get_gan_network` function defines and returns a Generative Adversarial Network (GAN) model that combines a generator and a discriminator to create an adversarial system."
      ],
      "metadata": {
        "id": "owuCp-HGVJ1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
        "    # We initially set trainable to False since we only want to train either the\n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = Input(shape=(random_dim,))\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "    # get the output of the discriminator (probability if the image is real or not)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return gan"
      ],
      "metadata": {
        "id": "RsXpr3xDU2RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `get_gan_network` function above creates and compiles a Generative Adversarial Network (GAN). It takes the discriminator, random noise dimension, generator, and optimizer as inputs. The function first sets the discriminator's weights to be non-trainable, ensuring that only the generator is updated when training the GAN. It then defines the input layer for the GAN, which accepts random noise vectors. This noise is passed through the generator to produce synthetic images. The generated images are then passed to the discriminator, which evaluates whether they are real or fake. The function creates a model that connects the input (random noise) to the output (discriminator's decision), compiles the model with a binary cross-entropy loss function and the provided optimizer, and finally returns the compiled GAN model, ready for training."
      ],
      "metadata": {
        "id": "C2MzUg5fXjg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Plotting the Generated Images**\n",
        "\n",
        "The `plot_generated_images` function below generates and saves a grid of synthetic MNIST images created by the generator. It takes several inputs: the current epoch number, the generator model, the number of images to generate (`examples`), the grid dimensions (`dim`), and the figure size (`figsize`). Inside the function, random noise vectors are generated and passed through the generator to create fake MNIST images. These images are reshaped to a 28x28 format and arranged in a grid based on the specified dimensions. The function then plots each image in the grid, disables the axis labels for a cleaner look, and saves the resulting image grid to a file with the epoch number in the filename. This allows tracking the progression of the generator's ability to produce realistic images as training progresses."
      ],
      "metadata": {
        "id": "-jGkCJvAZGmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a wall of generated MNIST images\n",
        "def plot_generated_images(epoch, generator, examples=20, dim=(10, 10), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"
      ],
      "metadata": {
        "id": "VGZHaLllU8JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Training and Evaluating the GAN**\n",
        "\n",
        "In the code below, the `train` function is responsible for training the GAN (Generative Adversarial Network) on the MNIST dataset. Here's a breakdown of what the code does:\n",
        "\n",
        "1. **Loading Data:** It first loads the MNIST dataset using `load_minst_data()`, which returns the training and testing data.\n",
        "\n",
        "2. **Batch Splitting:** The training data is divided into batches of size `batch_size` (default: 128). The `batch_count` is calculated based on the number of training samples divided by the batch size.\n",
        "\n",
        "3. **Model Initialization:** The optimizer (`adam`) is created using the `get_optimizer()` function. Then, the generator, discriminator, and GAN models are built using `get_generator()`, `get_discriminator()`, and `get_gan_network()` respectively.\n",
        "\n",
        "4. **Training Loop:** The training proceeds for the specified number of epochs:\n",
        "   - For each epoch, the function loops over batches of the training data.\n",
        "   - Random noise is generated to serve as input to the generator, and random real MNIST images are selected from the training data.\n",
        "   - **Discriminator Training:** The discriminator is trained on both real and generated images. The real images are labeled with values close to 1 (real), and the generated images are labeled as 0 (fake) with some label smoothing applied (real images are labeled 0.9 instead of 1).\n",
        "   - **Generator Training:** After training the discriminator, the generator is trained to fool the discriminator. The noise input is passed through the generator, and the output is labeled as real (`y_gen = np.ones(batch_size)`), but the discriminator is kept frozen (`trainable = False`) during this training.\n",
        "\n",
        "5. **Image Generation (Optional):** The commented-out section is for plotting generated images periodically during training. It would save images generated at the first epoch or every 20th epoch to monitor progress. In the final step, after all epochs, the `plot_generated_images()` function is called to save a final grid of generated images.\n",
        "\n",
        "In summary, the function implements the core GAN training process where the discriminator and generator are alternately trained. The discriminator learns to distinguish between real and fake images, while the generator tries to produce more realistic fake images over time. The result is the training of both models to improve the generator's ability to create convincing MNIST-like images."
      ],
      "metadata": {
        "id": "xSIvmCo2ZiVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=1, batch_size=128):\n",
        "    # Get the training and testing data\n",
        "    x_train, y_train, x_test, y_test = load_minst_data()\n",
        "    # Split the training data into batches of size 128\n",
        "    batch_count = x_train.shape[0] // batch_size\n",
        "\n",
        "    # Build our GAN netowrk\n",
        "    adam = get_optimizer()\n",
        "    generator = get_generator(adam)\n",
        "    discriminator = get_discriminator(adam)\n",
        "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
        "\n",
        "    for e in range(1, epochs+1):\n",
        "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for z in tqdm(range(batch_count)):\n",
        "            # Get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
        "\n",
        "            # Generate fake MNIST images\n",
        "            generated_images = generator.predict(noise)\n",
        "            X = np.concatenate([image_batch, generated_images])\n",
        "\n",
        "            # Labels for generated and real data\n",
        "            y_dis = np.zeros(2*batch_size)\n",
        "            # One-sided label smoothing\n",
        "            y_dis[:batch_size] = 0.9\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            y_gen = np.ones(batch_size)\n",
        "            discriminator.trainable = False\n",
        "            gan.train_on_batch(noise, y_gen)\n",
        "        #\n",
        "        #if e == 1 or e % 20 == 0:\n",
        "        #    plot_generated_images(e, generator)\n",
        "    #Plot using the final epoch\n",
        "    plot_generated_images(epochs, generator)"
      ],
      "metadata": {
        "id": "YXFTGUQUVDfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. Adjusting the Training**"
      ],
      "metadata": {
        "id": "8tc9TuvkZ2TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(2,128)"
      ],
      "metadata": {
        "id": "DdMEnyTrVT7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code `train(2, 128)` calls the `train` function with two arguments:\n",
        "\n",
        "1. **`epochs=2`**: This specifies that the training process should run for 2 epochs. An epoch means one complete pass through the entire training dataset. So, the model will go through the full MNIST dataset twice.\n",
        "\n",
        "2. **`batch_size=128`**: This specifies that the training will be done in batches of 128 samples at a time. During each batch, 128 random noise vectors and 128 real MNIST images are used to train the discriminator and the generator.\n",
        "\n",
        "In essence, `train(2, 128)` starts the GAN training process, running for 2 epochs with a batch size of 128. Each epoch consists of multiple iterations where the discriminator and generator are trained alternately. The model will learn to generate fake MNIST-like images, and the training process will print the progress of the epochs and batches. After the training, it will also generate and save images of the results at the end of the 2 epochs."
      ],
      "metadata": {
        "id": "3b9pUIQdZ9t_"
      }
    }
  ]
}