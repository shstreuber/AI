{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/AI/blob/main/CS_345_545_Midterm_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Welcome to the lab portion of your Midterm Exam!**\n",
        "\n",
        "In this lab, you will work with the IMDB Movie Review dataset which you have already encountered in Week 5 of this course. If you don't remember the dataset, please check out the February 6 and February 8 class session recordings in the [Zoom Cloud Recordings](https://applications.zoom.us/lti/rich/home).\n"
      ],
      "metadata": {
        "id": "6eam4D-6Pxaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**0. Importing the Required Libraries**"
      ],
      "metadata": {
        "id": "TpGXZWBISPMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loOxBh21PqrC"
      },
      "outputs": [],
      "source": [
        "# Here are the libraries\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Flatten, SimpleRNN, Embedding\n",
        "from keras.preprocessing import sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Importing and Inspecting the Data**\n",
        "The data in the IMDB dataset are already tokenized.\n",
        "\n",
        "This code will load the data, decode the integer-encoded reviews back to words using the word index, and then iterate through the training set to find the top 100 common and least common words. Finally, it will display these words. Adjust max_words if you want to display more or fewer words.\n",
        "\n",
        "**NOTE** that, in the interest of memory maintenance, we are loading only the first 5,000 words in the dataset and we are shortening each review to the first 100 words."
      ],
      "metadata": {
        "id": "ViC7JNN0gAwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset, but constrain what is being loaded so that you don't run out of memory,\n",
        "\n",
        "max_features = 5000  # Consider only the first 5,000 words in the dataset\n",
        "maxlen = 100  # Limit the length of each review to 100 words\n",
        "batch_size = 32\n",
        "\n",
        "# Now we can load the dataset into training and test sets with these parameters already in place.\n",
        "# That simplifies the preprocessing considerably.\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "metadata": {
        "id": "DwIferJSf3s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the word index from the dataset\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Reverse the word index to map integers to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "metadata": {
        "id": "L8Tfj-s7hvoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the frequency of each word in the dataset\n",
        "word_counts = {}\n",
        "for review in x_test:\n",
        "    for word in review:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "\n",
        "# Sort the words by their frequency in descending order\n",
        "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_words[0]"
      ],
      "metadata": {
        "id": "z8wkTooViIJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the top 100 most common and least common\n",
        "top_common_words = [reverse_word_index[word[0]] for word in sorted_words[:100]]\n",
        "top_uncommon_words = [reverse_word_index[word[0]] for word in sorted_words[-100:]]\n",
        "\n",
        "# Display the top 100 common and uncommon words\n",
        "print(\"Top 100 common Words:\")\n",
        "print(top_common_words)\n",
        "\n",
        "print(\"\\nTop 100 uncommon Words:\")\n",
        "print(top_uncommon_words)"
      ],
      "metadata": {
        "id": "ON2ziGJliLMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Preprocessing the Data for Neural Networks**"
      ],
      "metadata": {
        "id": "fNsjvbZgiPax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before we import the data, we are setting parameters for data preprocessing\n",
        "max_features = 5000  # Consider only the top 5,000 words in the dataset\n",
        "maxlen = 100  # Limit the length of each review to 100 words\n",
        "batch_size = 32\n",
        "\n",
        "# Now we can load the dataset into training and test sets with these parameters already in place.\n",
        "# That simplifies the preprocessing considerably.\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "# Pad sequences to ensure uniform length so we can analyze\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print(x_train[:5])"
      ],
      "metadata": {
        "id": "gWxgafzzSYyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Embedding Layer**\n",
        "\n",
        "The Embedding layer in a neural network is responsible for representing words or entities as dense vectors in a continuous vector space. It is commonly used in natural language processing (NLP) tasks, where words are represented as high-dimensional vectors.\n",
        "\n",
        "Here's how the Embedding layer works:\n",
        "\n",
        "1. **Word Representation**: Each word in a vocabulary is assigned a unique index. For example, in a vocabulary of 10,000 words, each word would have an index ranging from 1 to 10,000.\n",
        "\n",
        "2. **Embedding Matrix**: The Embedding layer initializes an embedding matrix, where each row corresponds to the vector representation of a word in the vocabulary. The size of this matrix is determined by the vocabulary size and the dimensionality of the embedding space.\n",
        "\n",
        "3. **Vector Lookup**: During training, the Embedding layer converts each word index in the input sequence into its corresponding dense vector representation by performing a lookup operation in the embedding matrix. This operation effectively transforms each word index into a dense vector of fixed size.\n",
        "\n",
        "4. **Learnable Parameters**: The parameters of the embedding matrix are learned during training through backpropagation. The network adjusts these parameters to minimize the loss function, effectively learning the most informative vector representations for the words in the vocabulary.\n",
        "\n",
        "5. **Semantic Similarity**: The learned dense vectors capture semantic relationships between words. Words that are semantically similar are likely to have similar vector representations, as they often appear in similar contexts within the training data.\n",
        "\n",
        "Overall, **the Embedding layer** plays a crucial role in converting discrete word indices into dense vector representations that capture semantic information, facilitating the learning process in tasks such as sentiment analysis, machine translation, and text generation."
      ],
      "metadata": {
        "id": "Hb47CG88kOsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Simple Feed-Forward/ Backpropagration Neural Network**\n",
        "In this section, we will build a regular feed-forward neural network"
      ],
      "metadata": {
        "id": "-Dn7Q66YTTjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile the feed-forward neural network model\n",
        "print('Build and compile feed-forward model...')\n",
        "FF_model = Sequential()\n",
        "FF_model.add(Embedding(max_features, 64))\n",
        "FF_model.add(Flatten())\n",
        "FF_model.add(Dense(64, activation='relu'))\n",
        "FF_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "FF_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9sAX13ZCTHeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feed-Forward Neural Network Model Architecture:\")\n",
        "FF_model.summary()"
      ],
      "metadata": {
        "id": "LIKrL7nTehdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the feed-forward neural network model\n",
        "\n",
        "print('Train feed-forward model...')\n",
        "FF_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "Ws18qrS8Xy1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the feed-forward neural network model\n",
        "print('Evaluate feed-forward model...')\n",
        "FF_acc = FF_model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Validation accuracy:', FF_acc)"
      ],
      "metadata": {
        "id": "edK7ruDeX9mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Recurrent Neural Network Model**"
      ],
      "metadata": {
        "id": "6DaDkqwQW9Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile the recurrent neural network model\n",
        "print('Build and compile recurrent neural network model...')\n",
        "RNN_model = Sequential()\n",
        "RNN_model.add(Embedding(max_features, 64))\n",
        "# RNN_model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "RNN_model.add(SimpleRNN(64))\n",
        "RNN_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "RNN_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "p-UhMXKNYEM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Recurrent Neural Network Model Architecture:\")\n",
        "RNN_model.summary()"
      ],
      "metadata": {
        "id": "nlx9IuPcewgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the recurrent neural network model\n",
        "print('Train recurrent neural network model ...')\n",
        "RNN_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "If0diWe4cchS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "print('Evaluate recurrent neural network model...')\n",
        "RNN_acc = RNN_model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Validation accuracy:', RNN_acc)"
      ],
      "metadata": {
        "id": "Flx7CRbOdDNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. LSTM Model**"
      ],
      "metadata": {
        "id": "w5DfDojVUoIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile the LSTM model\n",
        "print('Build and compile LSTM model...')\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(Embedding(max_features, 64))\n",
        "#LSTM_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "LSTM_model.add(LSTM(64))\n",
        "LSTM_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "LSTM_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EGfSn5UcTHt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LSTM Network Model Architecture:\")\n",
        "LSTM_model.summary()"
      ],
      "metadata": {
        "id": "8_kigXVge5-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print('Train LSTM model...')\n",
        "LSTM_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "pWnEMxelc-UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "print('Evaluate LSTM model...')\n",
        "LSTM_acc = LSTM_model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Validation accuracy:', LSTM_acc)"
      ],
      "metadata": {
        "id": "e13JuUD1dYsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**EXAMPLE CLASSIFICATION**\n",
        "Replace the text with the text you are given in the exam."
      ],
      "metadata": {
        "id": "wQ-AGCLYrBw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input sentence\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "text = \"[use the text from the exam]\"\n",
        "sequences = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "# Pad the sequence\n",
        "maxlen = 100  # Same length as the input expected by the model\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "Fj4fwNmRrF6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**PREDICT THE SENTIMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "_sLNIeIsr0RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(np.array(padded_sequences)) # replace model. with the name of the model you are using\n",
        "\n",
        "# Display the sentiment prediction\n",
        "if prediction[0] < 0.5:\n",
        "    print(\"Negative Sentiment\")\n",
        "else:\n",
        "    print(\"Positive Sentiment\")"
      ],
      "metadata": {
        "id": "-L-3Xo9zr4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Extra Credit Question (5 points)**\n",
        "To improve on the top 100 word list you are seeing in section 1 of this file, you could implement a stop word list. A stop word list literally stops common words (like \"and,\" \"it,\" \"but,\" etc. from appearing.\n",
        "\n",
        "Here is a code snippet for implementing stop words:\n",
        "\n",
        "```\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources (only required once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"This is an example sentence that contains stop words.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n",
        "```\n",
        "Use this snippet code as the basis to improve the quality of the data by filtering out the words that don't carry much meaning for our sentiment analysis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "US3mIM2pnCCc"
      }
    }
  ]
}