{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwN2LOn08UIWIYwIdWVeAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/AI/blob/main/Week7_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Problem with Feed-Forward Neural Networks**\n",
        "So far, you have used Neural Networks to analyze data and perform classifications based on their analysis--classifications of numbers, images, and the like.\n",
        "\n",
        "Remember the code below from [one of the very first neural networks you encountered in this class](https://github.com/shstreuber/AI/blob/main/Week1_EasyEquationNN5.ipynb)?\n",
        "\n",
        "**TO DO**: Execute this code 5 times in a row:"
      ],
      "metadata": {
        "id": "t_SeVjAFmpIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Sample Data\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-1.0,  1.0, 3.0, 5.0, 7.0, 9.0], dtype=float)\n",
        "\n",
        "# The Basic Model\n",
        "model = tf.keras.Sequential([tf.keras.Input([1]), keras.layers.Dense(units=1)])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "# Train the Model with 10 epochs\n",
        "model.fit(xs, ys, epochs=10)"
      ],
      "metadata": {
        "id": "8-Xt2t0RnM7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the outputs of all 5 runs. Does the learning improve from run 1 to run 5? How can we improve the the model? Record your thoughts below:"
      ],
      "metadata": {
        "id": "qzwprh0hpXN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sX23OUyqrla5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Solution: Recurrent Neural Networks**\n",
        "A recurrent neural network (RNN) is the type of artificial neural network that is used in Apple’s Siri and Google’s voice search. At a high level, a recurrent neural network (RNN) processes sequences — whether daily stock prices, sentences, or sensor measurements — one element at a time while retaining a **memory** (called a state) of what has come previously in the sequence.\n",
        "\n",
        "<img src=\"https://github.com/shstreuber/Data-Mining/blob/master/images/Structure-of-simple-recurrent-neural-network-RNN-and-unfolded-RNN.png?raw=true\">\n",
        "\n",
        "**How It Works**:\n",
        "1. **Inputs & Hidden State**: At each step, the RNN receives an input *x* and combines it with information from the previous step (stored in a hidden state *h*).\n",
        "2. **Recurrence**: This hidden state is passed to the next time step, allowing the network to retain context from earlier inputs.\n",
        "3. **Output**: The network processes all steps and produces an output, such as predicting the next word in a sentence or forecasting stock prices.\n",
        "\n",
        "##**Limitations**\n",
        "Simple RNN models usually run into two major issues. These issues are related to gradient, which is the slope of the loss function along with the error function.\n",
        "\n",
        "1. [Vanishing Gradient problem](https://aiml.com/what-do-you-mean-by-vanishing-and-exploding-gradient-problem-and-how-are-they-typically-addressed/) occurs when the gradient becomes so small that updating parameters becomes insignificant; eventually the algorithm stops learning:\n",
        "\n",
        "<img src =\"https://camo.githubusercontent.com/50601245fe1be8ab8a254056e6f7cd26ce2eb4c29dfe4adcc10363def93ae380/68747470733a2f2f7777772e6b646e7567676574732e636f6d2f77702d636f6e74656e742f75706c6f6164732f76616e697368696e672d6772616469656e742d70726f626c656d2d31322e706e67\" height =200>\n",
        "\n",
        "Imagine it like playing [silent post](https://actiwity.com/details/silentpost).\n",
        "\n",
        "2. [Exploding Gradient problem](https://aiml.com/what-do-you-mean-by-vanishing-and-exploding-gradient-problem-and-how-are-they-typically-addressed/) occurs when the gradient becomes too large, which makes the model unstable.\n",
        "<img src = \"https://aiml.com/wp-content/uploads/2023/11/vanishing-and-exploding-gradient-1.png\" height=200>\n",
        "\n",
        "In this case, larger error gradients accumulate, and the model weights become too large. This issue can cause longer training times and poor model performance. It is less common than the Vanishing Gradient.\n",
        "\n",
        "Watch this video to understand how these two concepts operate:"
      ],
      "metadata": {
        "id": "ZDOQJis8y7-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('qO_NLVjD6zE', width=600, height=400)"
      ],
      "metadata": {
        "id": "CATt7Mts7ix7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced RNN architectures such as LSTM and GRU mitigate the Vanishing Gradient problem. That's what we will see next."
      ],
      "metadata": {
        "id": "TifyTply77jA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MasterCard Stock Price Prediction Using LSTM & GRU**\n",
        "Below, we will use Kaggle’s MasterCard stock dataset from May-25-2006 to Oct-11-2021 to train an LSTM and a GRU model to forecast the stock price. As before, we will start with an Exploratory Data Analysis and data preprocessing and then build and train each model. We will also evaluate which model works best."
      ],
      "metadata": {
        "id": "wVP_2zcY25qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**0. Importing the Libraries and the Data**"
      ],
      "metadata": {
        "id": "5MGI6gYk3XNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT6Mhg51yy3h"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(455)\n",
        "np.random.seed(455)\n",
        "\n",
        "dataset = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/shstreuber/Data-Mining/refs/heads/master/data/Mastercard_stock_history.csv\", index_col=\"Date\", parse_dates=[\"Date\"]\n",
        ").drop([\"Dividends\", \"Stock Splits\"], axis=1)\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "GS-5INr1zIFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe()"
      ],
      "metadata": {
        "id": "Iqy-JsbL4Juv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "uzOXMon-4Ue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the data distribution\n",
        "tstart = 2016\n",
        "tend = 2020\n",
        "\n",
        "def train_test_plot(dataset, tstart, tend):\n",
        "    dataset.loc[f\"{tstart}\":f\"{tend}\", \"High\"].plot(figsize=(16, 4), legend=True)\n",
        "    dataset.loc[f\"{tend+1}\":, \"High\"].plot(figsize=(16, 4), legend=True)\n",
        "    plt.legend([f\"Train (Before {tend+1})\", f\"Test ({tend+1} and beyond)\"])\n",
        "    plt.title(\"MasterCard stock price\")\n",
        "    plt.show()\n",
        "\n",
        "train_test_plot(dataset,tstart,tend)"
      ],
      "metadata": {
        "id": "pIoUWWDv4iPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Preprocessing**\n"
      ],
      "metadata": {
        "id": "nvXhV-_m5cRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up Training and Test Sets\n",
        "def train_test_split(dataset, tstart, tend):\n",
        "    train = dataset.loc[f\"{tstart}\":f\"{tend}\", \"High\"].values\n",
        "    test = dataset.loc[f\"{tend+1}\":, \"High\"].values\n",
        "    return train, test\n",
        "training_set, test_set = train_test_split(dataset, tstart, tend)"
      ],
      "metadata": {
        "id": "dJzJ9z475q0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing the inputs with MinMaxScaler--this is a different form of normalization\n",
        "sc = MinMaxScaler(feature_range=(0, 1))\n",
        "training_set = training_set.reshape(-1, 1)\n",
        "training_set_scaled = sc.fit_transform(training_set)"
      ],
      "metadata": {
        "id": "mW56jLkS5xnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"This is the beginning of the Training Set BEFORE scaling \\n\",training_set)\n",
        "print(\"This is the beginning of the Training Set AFTER scaling \\n\",training_set_scaled)"
      ],
      "metadata": {
        "id": "oyWfT6oH57CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The split_sequence function uses a training dataset and converts it into inputs (X_train) and outputs (y_train).\n",
        "\n",
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()   # initialize two empty lists called X and y\n",
        "    for i in range(len(sequence)): # loop through the sequence argument and calculate the end_ix variable by adding the current index i to the n_steps argument\n",
        "        end_ix = i + n_steps\n",
        "        if end_ix > len(sequence) - 1: # If end_ix is greater than the length of the sequence minus 1, the loop is broken\n",
        "            break\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] # create two variables seq_x and seq_y by slicing the sequence from the current index i to end_ix and selecting the value at end_ix\n",
        "        X.append(seq_x) # append to the X and y lists\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y) # return X and y as numpy arrays\n",
        "\n",
        "\n",
        "n_steps = 60 # initialize with 60\n",
        "features = 1\n",
        "# split into samples\n",
        "X_train, y_train = split_sequence(training_set_scaled, n_steps) # assign the output of calling the split_sequence function with the training_set_scaled argument and n_steps variable"
      ],
      "metadata": {
        "id": "jSwZjY_X6pIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are working with a univariate series, so the number of features is one, and we need to reshape the X_train to fit on the LSTM model.\n",
        "# The X_train has [samples, timesteps], and we will reshape it to [samples, timesteps, features].\n",
        "\n",
        "# Reshaping X_train for model\n",
        "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],features)"
      ],
      "metadata": {
        "id": "i2SXB1IH7w2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. The LSTM Model**\n",
        "The Long Short Term Memory (LSTM) is an advanced type of RNN, designed to prevent both decaying and exploding gradient problems. Just like RNN, LSTM has repeating modules, but the structure is different. Think of an LSTM like a memory bank for the network. It has three parts: The Forget Gate, the Input Gate, and the Output Gate.\n",
        "\n",
        "This is what an LSTM model looks like:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1500/1*Mw4W7FZUbSr4EoriB5GuqQ.jpeg\">\n",
        "\n",
        "In an LSTM, the Forget Gate, Input Gate, and Output Gate work together to control the flow of information through the network. They use point-by-point multiplication and addition to manage how much information to keep, update, or output at each step in a sequence. Here's how each gate works:\n",
        "\n",
        "1. **Forget Gate**:\n",
        "The Forget Gate decides which information in the memory (the cell state) should be discarded.\n",
        "* It looks at the current input (data at the current time step) and the previous output (previous state).\n",
        "* It produces values between 0 and 1 using a sigmoid function. A value of 0 means \"forget everything,\" and 1 means \"keep everything.\"\n",
        "* Point-by-point multiplication: The output from the Forget Gate (values between 0 and 1) is multiplied element-wise with the previous cell state, controlling how much of the previous memory should be kept.\n",
        "2. **Input Gate**:\n",
        "The Input Gate decides what new information should be added to the memory.\n",
        "It has two parts:\n",
        "* A sigmoid activation that controls which values to update.\n",
        "* A tanh activation that creates new candidate values to add to the cell state.\n",
        "* Point-by-point multiplication: The output from the sigmoid gate multiplies element-wise with the new candidate values (from the tanh) to decide how much of the new information should be added to the memory.\n",
        "3. Output Gate:\n",
        "The Output Gate controls what part of the memory should be used to produce the current output.\n",
        "* It takes the current input and the updated memory (cell state) and passes them through a sigmoid function.\n",
        "* Then, it multiplies the output of the sigmoid function with the tanh of the updated cell state to decide the final output.\n",
        "* Point-by-point multiplication: The output from the sigmoid gate is multiplied element-wise with the updated cell state to produce the final output.\n",
        "\n",
        "<hr>\n",
        "\n",
        "Our model will consist of a single hidden layer of LSTM and an output layer. You can experiment with the number of units, as more units will give you better results. For this experiment, we will set LSTM units to 125, tanh as activation, and set input size.\n",
        "\n",
        "We will compile the model with an RMSprop optimizer and mean square error as a loss function."
      ],
      "metadata": {
        "id": "8OzkTzu_76dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.1 Build the Model**"
      ],
      "metadata": {
        "id": "ClCnYvx48aGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The LSTM architecture\n",
        "from tensorflow.keras import Input # This is to define the input layer\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Input(shape=(n_steps, features)))  # Explicitly defining the input layer\n",
        "model_lstm.add(LSTM(units=125, activation=\"tanh\"))\n",
        "model_lstm.add(Dense(units=1))\n",
        "\n",
        "# Compiling the model\n",
        "model_lstm.compile(optimizer=\"RMSprop\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_lstm.summary()\n"
      ],
      "metadata": {
        "id": "j-JKE8Rs73vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.2 Train the Model**"
      ],
      "metadata": {
        "id": "YTgMc7uy8e1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k_3skWZcEl41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2) # train on 50 epochs with 32 batch sizes."
      ],
      "metadata": {
        "id": "D9ko3uAd8TmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the loss value is very small, which indicates that the model is performing well on the training data."
      ],
      "metadata": {
        "id": "OkK9ak5T9AqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.3 Running the model on the Test Set**\n",
        "We are going to repeat preprocessing and normalize the test set. First of all, we will transform then split the dataset into samples, reshape it, predict, and inverse transform the predictions into standard form."
      ],
      "metadata": {
        "id": "wLKaDUta9VFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the \"High\" column from the dataset and assign it to the variable dataset_total\n",
        "dataset_total = dataset.loc[:, \"High\"]\n",
        "\n",
        "# Defining the Inputs\n",
        "# Select the last part of the dataset as inputs for the test set.\n",
        "# This takes values from dataset_total, starting from an index offset by the length of the test set and n_steps.\n",
        "inputs = dataset_total[len(dataset_total) - len(test_set) - n_steps:].values\n",
        "\n",
        "# Reshape inputs to a 2D array with one column to be used in the model\n",
        "inputs = inputs.reshape(-1, 1)  # Reshapes the data into a 2D array (one column)\n",
        "\n",
        "# Preprocessing with Scaling\n",
        "# Apply the scaling transformation to the inputs using the scaler (sc) to normalize the data\n",
        "inputs = sc.transform(inputs)  # Scale the inputs using the sc.transform() method\n",
        "\n",
        "# Splitting into samples\n",
        "# Split the sequence of inputs into samples for X_test (features) and y_test (labels)\n",
        "X_test, y_test = split_sequence(inputs, n_steps)\n",
        "\n",
        "# Reshape the input data for the model\n",
        "# The model expects 3D input (samples, time steps, features).\n",
        "# Reshape X_test into a 3D array where the number of samples is the first dimension,\n",
        "# n_steps (sequence length) is the second dimension, and the third dimension is the number of features.\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], features)\n",
        "\n",
        "# Predicting Stock Prices\n",
        "# Use the trained LSTM model to predict the stock prices based on the input sequences in X_test\n",
        "predicted_stock_price = model_lstm.predict(X_test)\n",
        "\n",
        "# Inverse scaling transformation\n",
        "# The model outputs scaled predictions, so we need to inverse transform the predicted values\n",
        "# using the scaler (sc) to obtain the actual stock price values\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n"
      ],
      "metadata": {
        "id": "SqJqOIzf9mrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.4 Plot Predictions vs. Actual Values from the Test Set**\n",
        "The code below defines two functions: `plot_predictions` and `return_rmse`. The `plot_predictions` function visualizes the real and predicted stock prices over time by plotting the values in a line graph, with the real values in gray and the predicted values in red. The `return_rmse` function calculates the Root Mean Squared Error (RMSE) between the actual and predicted stock prices, providing a measure of prediction accuracy. Finally, the code calls both functions to display the stock price predictions and the RMSE value for the test dataset and predicted stock prices."
      ],
      "metadata": {
        "id": "y5vRRbEc-iVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(test, predicted):\n",
        "    # This function takes in two arguments:\n",
        "    # 'test' represents the real stock price values (true values),\n",
        "    # 'predicted' represents the predicted stock price values by the model.\n",
        "\n",
        "    # Plotting the real stock prices in gray color\n",
        "    plt.plot(test, color=\"gray\", label=\"Real\")\n",
        "    # Plotting the predicted stock prices in red color\n",
        "    plt.plot(predicted, color=\"red\", label=\"Predicted\")\n",
        "\n",
        "    # Set the title of the plot\n",
        "    plt.title(\"MasterCard Stock Price Prediction\")\n",
        "\n",
        "    # Label the x-axis as 'Time' (since the data is over time)\n",
        "    plt.xlabel(\"Time\")\n",
        "\n",
        "    # Label the y-axis as 'MasterCard Stock Price' (the variable being predicted)\n",
        "    plt.ylabel(\"MasterCard Stock Price\")\n",
        "\n",
        "    # Show the legend to differentiate between real and predicted values\n",
        "    plt.legend()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def return_rmse(test, predicted):\n",
        "    # This function calculates and prints the Root Mean Squared Error (RMSE) between the actual and predicted stock prices.\n",
        "\n",
        "    # Use numpy to compute the RMSE, which gives an indication of the prediction accuracy\n",
        "    # RMSE is calculated as the square root of the mean squared error (MSE).\n",
        "    rmse = np.sqrt(mean_squared_error(test, predicted))\n",
        "\n",
        "    # Print the RMSE value with two decimal places for clarity\n",
        "    print(\"The root mean squared error is {:.2f}.\".format(rmse))\n",
        "\n",
        "# Call the plot_predictions function to visualize the real vs predicted stock prices\n",
        "plot_predictions(test_set, predicted_stock_price)\n",
        "\n",
        "# Call the return_rmse function to calculate and print the RMSE for the predictions\n",
        "return_rmse(test_set, predicted_stock_price)\n"
      ],
      "metadata": {
        "id": "zfMLEz83-hY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. GRU Model**\n",
        "\n",
        "The GRU architecture is similar to the LSTM (Long Short-Term Memory) model but simpler and faster.\n",
        "1. **Gates**: GRU has only two main gates that control the flow of information:\n",
        "- **Update Gate**: Decides how much of the previous information (memory) should be kept and how much should be replaced with new information. It’s like deciding how much of the past you want to remember and how much to forget.\n",
        "- **Reset Gate**: Decides how much of the previous memory should be discarded when considering the new input. It helps decide what past information should be ignored when making new predictions.\n",
        "2. **Memory Update**: GRU combines the new input with the memory from previous steps, using the update and reset gates. It keeps important past information and adds new insights from the current data.\n",
        "3. **Output**: The updated memory is then used to make predictions or output decisions, such as predicting the next value in a sequence.\n",
        "\n",
        "The GRU model is efficient because it simplifies the process compared to LSTM models, while still being able to capture important patterns in sequential data.\n",
        "\n",
        "Unlike LSTM, GRU does not have cell state Ct. It only has a hidden state ht, and due to the simple architecture, GRU has a lower training time compared to LSTM models. The GRU architecture is simpler as it takes input x(t) and the hidden state from the previous timestamp h(t-1) and outputs the new hidden state h(t).\n",
        "\n",
        "<img src =\"https://cdn-images-1.medium.com/max/1500/1*zFhmhw_SZcX4kUVQH-z2aw.jpeg\">\n",
        "\n",
        "We are going to keep everything the same and just replace the LSTM layer with the GRU layer so we can compare the results. The model structure contains a single GRU layer with 125 units and an output layer.\n"
      ],
      "metadata": {
        "id": "IOsXt-Ns_ZbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.1 Build the Model**"
      ],
      "metadata": {
        "id": "34T2M9Dt_2PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Input\n",
        "\n",
        "model_gru = Sequential()\n",
        "\n",
        "# Define the input shape using the Input() layer explicitly\n",
        "model_gru.add(Input(shape=(n_steps, features)))  # n_steps: number of time steps, features: number of features at each time step\n",
        "\n",
        "# Add GRU layer\n",
        "model_gru.add(GRU(units=125, activation=\"tanh\"))\n",
        "\n",
        "# Add Dense output layer\n",
        "model_gru.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model_gru.compile(optimizer=\"RMSprop\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Print the model summary\n",
        "model_gru.summary()\n"
      ],
      "metadata": {
        "id": "RI-G7H3S_jLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.2 Train the Model**"
      ],
      "metadata": {
        "id": "PqtBZedS_7ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "fSFt6vIY_qe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.3 Run the Model on the Test Set**"
      ],
      "metadata": {
        "id": "6JKu37gPADRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_predicted_stock_price = model_gru.predict(X_test)\n",
        "GRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)"
      ],
      "metadata": {
        "id": "XJOoCZaq_wth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.4 Plot Predictions vs. Actual Values from the Test Set**"
      ],
      "metadata": {
        "id": "YeaysmM9AOcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(test_set, GRU_predicted_stock_price)\n",
        "return_rmse(test_set,GRU_predicted_stock_price)"
      ],
      "metadata": {
        "id": "PV9B1DG0AVc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Task**: Which model performs better? What could be the reason? Record your thoughts below, then read on."
      ],
      "metadata": {
        "id": "6k__GesnJqvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5TcD6zjnJ1mG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in RMSE (Root Mean Squared Error) between the GRU and LSTM models could be due to several factors:\n",
        "\n",
        "**1. Model Complexity:**\n",
        "\n",
        "LSTM has a more complex architecture compared to GRU. Its three gates allow it to better capture long-term dependencies in the data. As a result, it may perform better at modeling sequential data, especially if the data has complex patterns or long-term relationships.\n",
        "GRU is simpler and has only two gates (update and reset), which can make it faster and easier to train but potentially less effective at capturing long-term dependencies.\n",
        "\n",
        "**2. Data Characteristics:**\n",
        "\n",
        "Our data may benefit more from the LSTM’s ability to handle long-term dependencies, especially since they  contain complex temporal relationships and trends over a long period. LSTM might be able to capture these patterns better. If the data is simpler or doesn't have many long-term dependencies, GRU may be sufficient, but it might not perform as well as LSTM.\n",
        "\n",
        "**3. Hyperparameters:**\n",
        "\n",
        "Differences in model performance can also be influenced by the choice of hyperparameters (e.g., number of units, learning rate, batch size). Even slight differences in how the models are trained can result in significant changes in performance.\n",
        "The GRU model might need further tuning in terms of the number of units, learning rate, or other training parameters to perform better.\n",
        "\n",
        "**4. Training and Optimization:**\n",
        "\n",
        "The optimizer (e.g., RMSprop) and training process for each model might be influencing the RMSE. LSTM models are generally more stable and easier to train for certain types of tasks, while GRU models might require additional tuning.\n",
        "\n",
        "**5. Overfitting or Underfitting:**\n",
        "\n",
        "If either model is overfitting or underfitting the training data, this can lead to a higher RMSE on the test set. You might want to check the validation loss and the generalization ability of the models to determine if this is the case."
      ],
      "metadata": {
        "id": "eyempdnLKCaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**5. Transformer Model**\n",
        "A [Transformer model](https://www.techtarget.com/searchenterpriseai/definition/transformer-model) is a type of machine learning model mainly used for tasks involving sequences, like language translation, text generation, or time series prediction. What makes it different from older models like LSTMs and GRUs is how it handles and processes data. A transformer architecture consists of an encoder and decoder that work together. The attention mechanism lets transformers encode the meaning of words based on the estimated importance of other words or tokens. This enables transformers to process all words or tokens in parallel for faster performance, helping drive the growth of increasingly bigger LLMs.\n",
        "\n",
        "Using the attention mechanism, the encoder block transforms each word or token into vectors further weighted by other words. For example, in the following two sentences, the meaning of it is weighted differently, owing to the change of the word filled to emptied:\n",
        "* He poured the pitcher into the cup and *filled* it.\n",
        "* He poured the pitcher into the cup and *emptied* it.\n",
        "\n",
        "The attention mechanism connects it to the cup being filled in the first sentence and to the pitcher being emptied in the second sentence.\n",
        "\n",
        "The decoder essentially reverses the process in the target domain:\n",
        "\n",
        "<img src = \"https://www.techtarget.com/rms/onlineimages/transformer_model_architecture-f.png\" width = 300>\n",
        "\n",
        "The important mechanisms are:\n",
        "1. **Self-Attention**:\n",
        "The key idea behind a Transformer is self-attention, which allows the model to look at all parts of the input sequence simultaneously (instead of step-by-step like LSTMs).\n",
        "This means the model can focus on important parts of the input, regardless of their position in the sequence. For example, in a sentence, the model can directly focus on relevant words, even if they're far apart.\n",
        "2. **Attention Mechanism**:\n",
        "The attention mechanism helps the model decide which parts of the input sequence should be given more importance when making predictions. It works like this:\n",
        "For each word (or time step), the model looks at all other words or time steps in the sequence and decides how much attention to pay to each one. This helps the model capture relationships between words or time steps, even if they're far apart.\n",
        "3. **Multi-Head Attention**:\n",
        "Multi-head attention means the model uses several \"attention heads,\" or separate attention mechanisms, to look at different parts of the sequence in parallel. This allows the model to capture various relationships in the data at once.\n",
        "4. **Feed-Forward Networks**:\n",
        "After the attention step, the Transformer uses a simple neural network (called a feed-forward network) to process the output further. This network helps the model make more complex decisions based on the attention results.\n",
        "5. **Positional Encoding**:\n",
        "Unlike older models like LSTMs, the Transformer doesn't process the data in a sequence. Instead, it processes all parts at once. To keep track of the order of the sequence (e.g., the order of words in a sentence or time steps in a series), positional encoding is added, which tells the model the position of each word or time step in the sequence.\n",
        "\n",
        "How It Works:\n",
        "* **Input**: The input sequence (like a sentence or a time series) is passed into the Transformer.\n",
        "* **Self-Attention**: The model looks at the entire sequence and determines which parts are most important to focus on at each step.\n",
        "* **Processing**: The model then processes the sequence through multiple layers of attention and feed-forward networks.\n",
        "* **Output**: After processing the sequence, the model outputs its prediction (like translating a sentence or predicting a future value).\n",
        "\n",
        "**Why Transformers Are Powerful:**\n",
        "\n",
        "1. **Efficiency**: Transformers can process the entire sequence at once, unlike models like LSTMs, which process one step at a time. This makes Transformers faster, especially for long sequences.\n",
        "2. **Long-Term Dependencies**: The attention mechanism allows Transformers to capture long-range dependencies more easily than LSTMs or GRUs, which can struggle with long sequences.\n",
        "3. **Scalability**: Transformers can handle large datasets and are often used in very complex tasks, such as language models like GPT (the model you're interacting with right now).\n",
        "\n",
        "In short, Transformers are great at understanding complex patterns in sequences by focusing on important parts of the data, making them ideal for tasks like text processing and time series forecasting.\n",
        "\n",
        "In our code example, we’ll replace the LSTM/GRU layers with Transformer blocks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7HaRkYTKPmo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5.1 Build the Model**\n"
      ],
      "metadata": {
        "id": "tWZ25fnXTuJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define Transformer block (Self-Attention Layer)\n",
        "def transformer_block(inputs, num_heads, num_units, dropout_rate=0.3):\n",
        "    # Multi-head self-attention layer\n",
        "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=num_units)(inputs, inputs)\n",
        "    attention = Dropout(dropout_rate)(attention)\n",
        "    attention = LayerNormalization()(attention)\n",
        "\n",
        "    # Feed-forward network (Fully connected) for processing attention output\n",
        "    ffn = Dense(num_units * 2, activation='relu')(attention)  # Increased the size of the dense layer\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "    ffn = LayerNormalization()(ffn)\n",
        "\n",
        "    return ffn\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(n_steps, features))\n",
        "\n",
        "# Apply transformer block with modified hyperparameters\n",
        "x = transformer_block(input_layer, num_heads=16, num_units=128, dropout_rate=0.3)\n",
        "\n",
        "# Add another Transformer block for deeper learning\n",
        "x = transformer_block(x, num_heads=16, num_units=128, dropout_rate=0.3)\n",
        "\n",
        "# Flatten the output for the Dense layer\n",
        "x = Flatten()(x)\n",
        "\n",
        "# Dense output layer for regression (predicting stock price)\n",
        "output_layer = Dense(1)(x)\n",
        "\n",
        "# Create the model using the functional API\n",
        "model_transformer = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "optimizer = Adam(learning_rate=0.001)  # Added a learning rate setting\n",
        "model_transformer.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "# Add early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "# Print the model summary\n",
        "model_transformer.summary()\n"
      ],
      "metadata": {
        "id": "Lxxx_b_Ccjxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the Code**:\n",
        "1. **Transformer Block**:\n",
        "The TransformerBlock class defines a single layer of the Transformer architecture. It uses the MultiHeadAttention layer to capture dependencies between different time steps in the input sequence.\n",
        "The feed-forward network (FFN) processes the output of the attention mechanism, and LayerNormalization and Dropout are applied for stability and regularization.\n",
        "\n",
        "2. **Model Definition**: The build_transformer_model function defines the Transformer model architecture.\n",
        "The input layer accepts sequences of shape (n_steps, features). We stack multiple Transformer blocks (defined by the num_layers parameter). After processing through the Transformer layers, the output is pooled using GlobalAveragePooling1D to reduce the sequence to a fixed-size vector. Finally, a Dense layer is added for regression (or forecasting), with a single output neuron (since we are predicting a continuous value).\n",
        "3. **Compilation**: The model is compiled with RMSprop optimizer and Mean Squared Error (MSE) loss for regression.\n"
      ],
      "metadata": {
        "id": "CVvF84s7TmnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2 Train the Model**\n",
        "We train the model in the same way we trained the LSTM or GRU model, adding validation metrics and monitoring loss."
      ],
      "metadata": {
        "id": "eWme8pYdU26M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_transformer.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "lvESm7GvUyo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5.3 Run the Model on the Test Set**"
      ],
      "metadata": {
        "id": "s1mXEYuoTme7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Generate predictions from the Transformer model\n",
        "transformer_predicted_stock_price = model_transformer.predict(X_test)\n",
        "\n",
        "# Step 2: Inverse transform the predicted values to get the actual stock prices\n",
        "transformer_predicted_stock_price = sc.inverse_transform(transformer_predicted_stock_price)"
      ],
      "metadata": {
        "id": "zxrGbUNtYpTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5.3 Plot Predictions vs. Actual Values from the Test Set**"
      ],
      "metadata": {
        "id": "veP2hAkUY4xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Visualize the real vs predicted stock prices\n",
        "plot_predictions(test_set, transformer_predicted_stock_price)\n",
        "\n",
        "# Step 4: Calculate and print the RMSE for the predictions\n",
        "return_rmse(test_set, transformer_predicted_stock_price)"
      ],
      "metadata": {
        "id": "lKz3QzNjZJUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6. Exercises**"
      ],
      "metadata": {
        "id": "uQJe2Bm-MzFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6.1: RNNs**\n",
        "Tune the code for the LSTM and the GRU models such that their performance becomes similar or near identical. You can play with any of the following:\n",
        "1. Hyperparameters\n",
        "2. Input parameters/ number of features\n",
        "3. Model layers and architecture\n",
        "4. Anything else\n",
        "\n",
        "When you are satisfied, copy the revised code in the field(s) below."
      ],
      "metadata": {
        "id": "vAVgqf-zi3ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the tuned LSTM code here"
      ],
      "metadata": {
        "id": "kaBufcQDNYJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the tuned GRU code here"
      ],
      "metadata": {
        "id": "X-A8F0mZNqiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6.2: Transformer and Query Engineering**\n",
        "The output for the Transfoermer model points to problems with the model architecture itself. Use any TWO of the following to improve the model performance and bring the RMSE under 100. NOTE that you must use IDENTICAL queries for both AIs:\n",
        "* OpenAI's [ChatGPT](https://chat.openai.com/) (GPT)\n",
        "* Google's [Aistudio](https://aistudio.google.com/prompts/new_chat) (Gemini)\n",
        "* Meta [AI Assistant](https://www.meta.ai/) (LLama)\n",
        "* xAI [Grok](https://x.ai/grok) (the Tesla of AIs)\n",
        "* Perplexity's [AI](https://www.perplexity.ai/) (another GPT)\n",
        "* Microsoft [Copilot](https://copilot.microsoft.com/) (Mistral)\n",
        "\n",
        "\n",
        "State below which two systems you have used, then paste your query history into the text fields, paste the best code solution into the code field, and in at least two sentences, evaluate which AI has provided the best solution most easily.\n"
      ],
      "metadata": {
        "id": "bApXI1J_izmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Systems I have used are:"
      ],
      "metadata": {
        "id": "CVwvPboUrWlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My query history:"
      ],
      "metadata": {
        "id": "RCssY6NBrjFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The winning code solution"
      ],
      "metadata": {
        "id": "q-hcovqgrpVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best AI for this coding job was ____ because ___ ..."
      ],
      "metadata": {
        "id": "6Lc2hFcSr2P9"
      }
    }
  ]
}