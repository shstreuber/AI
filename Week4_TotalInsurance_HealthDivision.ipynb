{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNX7zrRfe88rm2ttlc23ZEy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/AI/blob/main/Week4_TotalInsurance_HealthDivision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Job**\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/totalinsurance.jpg\" width=\"300\">\n",
        "</div>\n",
        "\n",
        "You work for TotalInsurance, an insurance carrier for home, health, and vehicles. Their Health Division has decided to automate their predictive processes in order to gain faster (ideally, real-time) insight into their customer data, so that health insurance claims can be approved or denied automatically and only go to an analyst for review if the customer requests manual review. In addition, TotalInsurance is automating its claim forecasting and its regional office staffing. You have received a small excerpt of the data in order to build a Deep Learning proof-of-concept.\n",
        "\n",
        "If you succeed, TotalInsurance will give you a 1,000 bonus."
      ],
      "metadata": {
        "id": "8bnu1AxSoMbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Process**\n",
        "We will be following the basic classification steps:\n",
        "\n",
        "0. Preparation, loading libraries and data\n",
        "1. Exploratory Data Analysis (EDA) to see how the data is distributed and to determine what the label should be. This will be the label you'll predict later on\n",
        "2. Preprocess the data (remove n/a, transform data types as needed, deal with missing data) --> here is where we will need to take a few additional steps to configure our data for the Neural Network\n",
        "3. Split the data into a training set and a test set\n",
        "4. Build the model based on the training set\n",
        "5. Test the model on the test set\n",
        "6. Determine the quality of the model with the help of a Confusion Matrix and a Classification Report.\n",
        "\n",
        "To understand what each step does, please look at the code comments and explanations\n",
        "\n"
      ],
      "metadata": {
        "id": "v_p147-MQAnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**0. Preparation**\n",
        "We will build our Deep Learning architecture on Tensorflow. Why Tensorflow? Because it is easier to build on Colab than Pytorch (for more about the battle of the giants, i.e. Tensorflow vs Pytorch, [read here](https://))."
      ],
      "metadata": {
        "id": "PAfeogrEUUcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyKMNeVHP5u4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # This tells Colab that we are using TensorFlow\n",
        "\n",
        "from tensorflow import keras # This is the main TensorFlow library\n",
        "from tensorflow.keras.models import Sequential # We are building a model that runs its layers in sequential order\n",
        "from tensorflow.keras import layers # We are building a Neural Network with several hidden layers\n",
        "from tensorflow.keras.layers import Dense # This will help us build a fully connected architecture\n",
        "#from tensorflow.keras.layers import preprocessing #\n",
        "from tensorflow.keras.layers import Normalization, Rescaling\n",
        "\n",
        "print(\"Current TensorFlow version is\", tf.__version__)\n",
        "\n",
        "import numpy as np # Your basic mathematical library for big datasets\n",
        "import pandas as pd # The library you need in order to clean and manipulate big datasets\n",
        "import matplotlib.pyplot as plt # Makes pretty pictures\n",
        "import seaborn as sns # for visualization aka more pretty pictures\n",
        "from sklearn.model_selection import train_test_split # Scikit-Learn is the default data science library\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42) # Setting a seed value for the randomizer so we get repeatable results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading in the data as fraud dataframe\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance_full2.csv\")\n",
        "insurance.head(10) # Let's look at the first 10 rows"
      ],
      "metadata": {
        "id": "875KtF4YNOLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Exploratory Data Analysis (EDA)**\n",
        "The goal of exploratory data analysis (EDA) is to visually and statistically summarize the main characteristics of a dataset, understand its structure, identify patterns, and detect anomalies. Through EDA, we can gain insights into the data, generate hypotheses, and determine the next steps of analysis or modeling.\n",
        "\n",
        "Exploratory data analysis involves techniques such as summary statistics, data visualization, and graphical representations to reveal hidden patterns, relationships, or trends within the data. EDA is an essential preliminary step in the data analysis pipeline, helping to uncover meaningful information and guide further exploration or hypothesis testing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vh0oeQbRNZNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Any missing values?\n",
        "insurance.isna().sum()"
      ],
      "metadata": {
        "id": "AxF_pGzl3vXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What data types and input features do we have? What could be our output label? Do we already have a label that contains the information, or do we need to create one?\n",
        "insurance.dtypes"
      ],
      "metadata": {
        "id": "UnEk4K33PQKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look more closely at all data.\n",
        "insurance.describe(include = 'all'), print(\"***DATA OVERVIEW***\") # Build a data summary for ALL data in the set (not just numeric!)"
      ],
      "metadata": {
        "id": "tOjMCIp1NUm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Findings**\n",
        "0. **MISSING DATA?** No. All fields have 15420 data points.\n",
        "1. **POSSIBLE TARGET VARIABLES**:\n",
        "  \n",
        "    **- BINARY** (Classification): insuranceclaim--whether approved or denied\n",
        "    \n",
        "    **- CATEGORICAL** (Classification): region, which has 4 levels. We could predict in which region a client lives, which could inform our decisions on how high to set insurance rates\n",
        "\n",
        "    **- NUMERIC** (Regression): Charges. We can predict the amount in which a client will submit a claim.\n",
        "\n",
        "2. **QUESTIONS TO ASK**: Which input attributes are relevant to our target variable? Which input attributes are not relevant?\n",
        "3. **ANY PROBLEMATIC DATA?**: No\n",
        "4. **ANY INCONSISTENT DATA?**: No\n",
        "5. **ANY OPPORTUNITIES FOR SIMPLIFYING?**: No\n",
        "6. **ANY OPPORTUNITIES FOR REDUCING THE DATAFRAME FOR EASE OF PROCESSING IN A NEURAL NETWORK?**: No"
      ],
      "metadata": {
        "id": "B3-NZ8gNSG_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2A. Data Cleanup**\n",
        "This is a clean dataset with no missing or incongruent values. No substitution for missing values or unusual values is needed."
      ],
      "metadata": {
        "id": "tAML7aPIbZWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2B. Preprocessing**\n",
        "Preprocessing means formatting the data such that it can work with the selected algorithm(s). This can involve\n",
        "* Substituting values to create a uniform data type\n",
        "* Transforming data from string to categorical or numeric or vice versa\n",
        "* Binning and bucketing data as needed (i.e. creating roll-up attributes in which we trade accuracy of values for ease of processing)\n",
        "\n",
        "And others"
      ],
      "metadata": {
        "id": "_a2EjJz-zF2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurance.dtypes"
      ],
      "metadata": {
        "id": "fYABlLE60g5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2C: Preparing the Data for Use with Tensorflow**\n",
        "There are 4 steps we need to take to prepare the data to run with TensorFlow (before we even consider the architecture of the network):\n",
        "\n",
        "1. Setting up training and test set\n",
        "2. Splitting features from labels (to build the input and output layers)\n",
        "3. Encoding categorical variables\n",
        "4. Normalize all numeric features"
      ],
      "metadata": {
        "id": "IT4_LeEVLRXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1. Setting up training and test set**\n",
        "There are different ways to split the data into a training and test set. You can specify a split by line indexes, by percentages, or by number of rows. In our example, we will use percentages to split."
      ],
      "metadata": {
        "id": "4CQHuS7eS4Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = insurance.sample(frac=0.8, random_state=0) # training dataset is 80%, test dataset is 20%. Rows are picked by random sampling\n",
        "test_dataset = insurance.drop(train_dataset.index) # Dropping the index numbers because we want the test set to be autonomous"
      ],
      "metadata": {
        "id": "wEq2nULwTidX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.head()"
      ],
      "metadata": {
        "id": "4r3neAiqUKqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.head()"
      ],
      "metadata": {
        "id": "k-4GABlPUQXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2. Splitting Features from Labels**\n",
        "Separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict--in our case, we want to predict insuranceclaim."
      ],
      "metadata": {
        "id": "FWInxsjmUf_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "train_labels = train_features.pop('insuranceclaim')\n",
        "test_labels = test_features.pop('insuranceclaim')"
      ],
      "metadata": {
        "id": "DFdYUeiKUsBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1. One Hot Encoding**\n",
        "Remember that the input layer for a Neural Network requires numeric information only.\n",
        "\n",
        "So, to help our computer understand, for example, the smoker variable, we use one-hot encoding. We create two slots: one for yes and one for no. When we see a smoker, we put a 1 in the yes slot and 0 in the no slot. If the client is a nonsmoker, we put a 1 in the no slot and 0 in the yes slot.\n",
        "\n",
        "So, in a nutshell, we use Encoding when:\n",
        "\n",
        "* The categorical features present in the data are not ordinal\n",
        "* When the number of categorical features present in the dataset is small so that the one-hot encoding technique can be effectively applied while building the model.\n",
        "\n",
        "We should not use One Hot Encoding when:\n",
        "\n",
        "* The categorical features present in the dataset are ordinal i.e for the data being like Junior, Senior, Executive, Owner.\n",
        "* When the number of categories in the dataset is quite large. Working with large categories can lead to high memory consumption and to processor issues (how does Professor Streuber know? Ask her for her story!)\n",
        "\n",
        "**NOTE:** This is an older dataset which recognizes sex as only male and female; we now recognize a broader spectrum of gender identities."
      ],
      "metadata": {
        "id": "VvD9DgMbMUre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using One-Hot Encoding with pd.getdummies\n",
        "train_features = pd.get_dummies(train_features, columns=['sex','smoker','region'], prefix='', prefix_sep='')\n",
        "# This will give us a boolean output with True and False. In order to continue, we will need to convert the output to 1 (True) and 0 (False)\n",
        "\n",
        "# Here is the conversion to integers)\n",
        "train_features = train_features.astype(int)\n",
        "\n",
        "# Let's see if we got this right\n",
        "train_features.head()"
      ],
      "metadata": {
        "id": "IdxEMA3gMPnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = pd.get_dummies(test_features, columns=['sex','smoker','region'], prefix='', prefix_sep='')\n",
        "test_features = test_features.astype(int)\n",
        "test_features.head()"
      ],
      "metadata": {
        "id": "vE2cQ__uU_eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.get_dummies(train_labels, columns=['insuranceclaim'], prefix='', prefix_sep='')\n",
        "train_labels = train_labels.astype(int)\n",
        "train_labels.head()"
      ],
      "metadata": {
        "id": "LB9kd-UWVKME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = pd.get_dummies(test_labels, columns=['insuranceclaim'], prefix='', prefix_sep='')\n",
        "test_labels = test_labels.astype(int)\n",
        "test_labels.head()"
      ],
      "metadata": {
        "id": "J54jJNAolBnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**4. Normalize all NUMERIC features**\n",
        "**Why should we normalize?**\n",
        "Well, not only do Neural Networks not like string-type labels in the output layer; they also don't like non-standardized input attributes (aka features). That's because the Summation and Activation functions treat the values from each input attribute the same. Hence, if these values fall into the same scale, the outcome of our classification will be better. That is why will want to normalize our feature values.\n",
        "\n",
        "There are different ways of normalizing data. One way is to do the math manually as you have seen in the previous week's file.\n",
        "\n"
      ],
      "metadata": {
        "id": "cYC4r5XOWh8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way is to use the [**preprocessing.Normalization layer**](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). This layer is a clean way to build that preprocessing into your model. It does, however, not like anything that doesn't fall into a numpy array, so be sure to one hot encode all your categorical variables or transform them into numerics."
      ],
      "metadata": {
        "id": "1goMn-rJKSPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Normalization layer\n",
        "normalizer = Normalization(axis=-1) # Here we set the normalizer up\n",
        "\n",
        "# Fit the normalizer to the training data\n",
        "normalizer.adapt(np.array(train_features)) # Now we apply the normalizer to the data. This calculates the mean and variance and stores them in the layer.\n",
        "\n",
        "# Print the mean and variance calculated by the normalizer\n",
        "print(\"Mean:\", normalizer.mean.numpy())\n",
        "print(\"Variance:\", normalizer.variance.numpy())\n",
        "\n",
        "# When the layer is called, it returns the normalized data\n",
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "    print('Original data:', first)\n",
        "    print()\n",
        "    print('Normalized data:', normalizer(first).numpy())"
      ],
      "metadata": {
        "id": "UJVlh-qvYja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to also apply the preprocessing.Normalization layer to our test set\n",
        "\n",
        "normalizer = Normalization(axis=-1) # Here we set the normalizer up\n",
        "normalizer.adapt(np.array(test_features)) # Now we apply the normalizer to the data. This calculates the mean and variance and stores them in the layer.\n",
        "\n",
        "# Print the mean and variance calculated by the normalizer\n",
        "print(\"Mean:\", normalizer.mean.numpy())\n",
        "print(\"Variance:\", normalizer.variance.numpy())\n",
        "\n",
        "# When the layer is called it returns the input data, with each feature independently normalized:\n",
        "first = np.array(test_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('Original data:', first)\n",
        "  print()\n",
        "  print('Normalized data:', normalizer(first).numpy())"
      ],
      "metadata": {
        "id": "0xD-7nz8El45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Building the Model**\n",
        "There is always a specific process with which to build a TensorFlow model:\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/TF_Process2.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "1. First, we set up the **keras SEQUENTIAL MODEL**. This is the framework inside of which we are going to define the layers. Sequential = layers are sequentially next to each other (either “stacked” or left-to-right, depending on how you draw them).\n",
        "---\n",
        "2. Inside the Sequential model, we define the **LAYERS**. To do this, we need to know the following:\n",
        "* **Normalization Layer**: If used, this is usually the first layer when input data needs to be normalized.\n",
        "* **Shape**: This is the number of attributes we use as input for the model.\n",
        "We need to ensure that the input layer has the correct number of input features. This can be specified when creating the first layer with the input_shape argument.\n",
        "---\n",
        "3. In the next step, we define HOW we want the model to run, that is to **COMPILE**, with model.compile(). To do this, we need to know the following:\n",
        "* **Optimizer** = gradient descent function (i.e. which function we use to optimize the step-down of the weights); adam = adaptive learning rate optimization algorithm\n",
        "* **Loss Function**= evaluation of the ŷ vs the ground truth\n",
        "* **Metrics** = evaluation criterion, typically accuracy.\n",
        "---\n",
        "4. Then, we **FIT** the model to the training set with model.fit(). To do this, we need to know the following:\n",
        "* **Epoch**: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. If one epoch is too big to feed to the computer at once we can divide it in several smaller batches\n",
        "* **Batch size**: Depending on the number of needed features in your dataset (you should reduce these to NO MORE THAN 6), the computing effort can be too intense. Just like you would not each a whole sandwich in one bite, the machine does better when processing the data in smaller bites called batches. The standard batch size is 32.\n",
        "---\n",
        "5. Lastly, we use our model to **PREDICT** the values for the test set with model.predict()\n",
        "---\n",
        "**How we choose the LOSS FUNCTION** for step 3 depends on the type of calculation we need our Neural Network to perform:\n",
        "* If the output variable is **continuous**, we are performing a regression, so the loss function is **mean squared error or MSE**\n",
        "* If the output variable is **binary**, we are performing a classification, so the loss function is **binary_crossentropy**\n",
        "* If the output variable is **categorical** with more than two labels, we are still performing a classification, but now the loss function is **categorical_crossentropy**\n",
        "\n",
        "**How we choose the ACTIVATION FUNCTION** when defining the layers: It used to be the case that Sigmoid and Tanh activation functions were preferred for all layers. These days, better performance is achieved using the  [relu function](https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning). Using a sigmoid on the output layer ensures your network output is between 0 and 1 and is easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
        "\n",
        "**How we choose the NUMBER AND SIZE OF LAYERS**:\n",
        "\n",
        "The short answer is: We experiment until we get the best output the fastest. The longer answer is: We can use various optimization strategies that can help us out somewhat. So, let's assume that trial and error has shown us that three layers is optimal. Furthermore, let's assume that we are going to build a Dense Network, aka a fully connected network structure, in which every node is connected with every node in the next layer.\n",
        "\n",
        "To define this architecture, we will specify the number of neurons or nodes in the layer as the first argument, and set up the activation function with the activation argument.\n",
        "\n",
        "As activation function, we will use the rectified linear unit or ReLU activation function on the first two layers and the [the sigmoid function ](https://towardsdatascience.com/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9) in the output layer since our output is binary.\n",
        "\n"
      ],
      "metadata": {
        "id": "H0ASsd3aQ5F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.1 Defining the keras model**\n",
        "We will build our model as follows:\n",
        "1. Use keras.Sequential\n",
        "2. If we have any numeric data, add our normalizer layer\n",
        "3. Add two hidden layers with 24 nodes each; we will use the [relu function](https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning) so that all positive values will remain positive but all negative values will become 0.\n",
        "4. For the output layer, we will use [the sigmoid function ](https://towardsdatascience.com/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9) since our output is binary.\n",
        "\n"
      ],
      "metadata": {
        "id": "u8bZkWBcZ-CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(24, input_shape=(11,), activation='relu')) # We have 11 columns in the encoded training dataset\n",
        "model.add(Dense(24, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "d4IN9loNlf3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.2 Compiling the model**\n",
        "Now we can configure the training procedure using the Model.compile() method. The most important arguments to compile are the loss and the optimizer since these define what will be optimized (binary_crossentropy) and how (using the [optimizers.Adam](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.tensorflow.org%2Fapi_docs%2Fpython%2Ftf%2Fkeras%2Foptimizers%2FAdam)). Note that we can adjust the learning_rate, which helps us tune the gradient."
      ],
      "metadata": {
        "id": "AAqfRAzVhDhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BOcKJAgYhSbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.3 Training the model**\n",
        "Once the model is configured, we use Model.fit() to train it:"
      ],
      "metadata": {
        "id": "aUTuwvFnhnpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=10,\n",
        "    # suppress logging\n",
        "    verbose=1,\n",
        "    # Calculate validation results on 20% of the training data. Validation means that we test as we go, on a 20% subset of the training data\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "z3z_HrEmhzJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Evaluating the model**\n",
        "We have trained our neural network and we can now evaluate the performance of the network on the test dataset. To evaluate your model on your training dataset, we can either use the predict() function to see the individual predictions for our test set, or we can use the evaluate() function and pass it the test data.\n",
        "\n",
        "The evaluate() function will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy. The function will return a list with two values. The first will be the loss of the model on the dataset and the second will be the accuracy of the model on the dataset.\n"
      ],
      "metadata": {
        "id": "vgmdUgMEpv0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_features, test_labels)"
      ],
      "metadata": {
        "id": "c89nN9zop_Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**EXERCISES**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Le-NKr-oPx_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Try the model out as a regression**\n",
        "Use \"charges\" as your target variable. The activation function for the output layer will be \"linear\""
      ],
      "metadata": {
        "id": "L2O-rJTClv71"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5nBKWJ7TmM-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Try the model out as a classification**\n",
        "Use \"region\" as your target variable. The activation function for the output layer will be \"softmax\""
      ],
      "metadata": {
        "id": "VYqYcSg-mLN9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-ec_zGunf4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Optimize the gradient descent**\n",
        "As you can see, the binary model overfits when run with a learning rate of 0.1. Edit the model and set the learning rate as follows:\n",
        "1. 0.01\n",
        "2. 0.001\n",
        "3. 0.5\n",
        "\n",
        "What do you observe?"
      ],
      "metadata": {
        "id": "wpyiS2AEms5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_b5zLUn6oG7N"
      }
    }
  ]
}