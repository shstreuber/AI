{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "natural-language-processing-tensorflow",
      "graded_item_id": "3n7IN",
      "launcher_item_id": "I08yU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/AI/blob/main/WEEK5_BBC_ExerciseV2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnwiOnGyW5JK"
      },
      "source": [
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from os import getcwd\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYw-e9pDGdL_"
      },
      "source": [
        "This Python notebook is from *Exercise 2 BBC News Archive* in the Coursera class *Natural Language Processing in Tensorflow*.\n",
        "\n",
        "Adapted by Jung Hee Kim and Michael Glass.\n",
        "\n",
        "## Use word embeddings and a neural network to classify BBC news articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaVpWZetzfpv"
      },
      "source": [
        "# Fetch the dataset of BBC news articles, with six different categories\n",
        "# You can read about this dataset here: https://www.kaggle.com/yufengdev/bbc-fulltext-and-category\n",
        "#\n",
        "!wget https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\n",
        "path_bbc = \"bbc-text.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYo6A4v5ZABQ"
      },
      "source": [
        "# Here are parameters that you can adjust for this lab.\n",
        "#\n",
        "# Parameters for producing sequences of number-tokens\n",
        "#\n",
        "vocab_size = 1000   # Number of most-common vocabulary words to recognize\n",
        "max_length = 120    # Standardized length (in word-tokens) of one article\n",
        "trunc_type = 'post' # Longer articles are truncated after max_length words\n",
        "padding_type = 'post' # Shorter articles are padded at the right end\n",
        "oov_tok = '<oov> '  # Uncommon words are replaced with this fake-word\n",
        "\n",
        "# For splitting between training and testing\n",
        "training_portion = .8\n",
        "\n",
        "# Size of embedding vectors for neural network processing\n",
        "embedding_dim = # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77fCI5gx7ZgR"
      },
      "source": [
        "#  Function to prepare the text of an article.\n",
        "#  Right now it simply:\n",
        "#    -- makes lower-case\n",
        "#    -- removes extra white-space, mulitple spaces are condensed to single space\n",
        "#\n",
        "#  You can also update the function to return the article with stop words removed\n",
        "#\n",
        "def prepareText(s):\n",
        "  # Lowercase\n",
        "  t = s.lower()\n",
        "  # Split into list of words.  This will eliminate all the white space.\n",
        "  tlist = t.split()\n",
        "  #\n",
        "  # CODE FOR REMOVING STOPWORDS HERE.  Remove comment.\n",
        "  #  tlist = [w for w in tlist if w not in stopwords]\n",
        "  #\n",
        "  # Then rejoin into a single article string, with a single space between words\n",
        "  t = ' '.join(tlist)\n",
        "  return t\n",
        "\n",
        "#\n",
        "#  Stopwords are very common words, which often do not contribute any\n",
        "#  information to the task at hand. They are often deleted.\n",
        "#  This is a common list of stop words.\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "# Test the prepareText function\n",
        "#\n",
        "testArticle = \"I read it in  The  Times of London\"\n",
        "print(prepareText(testArticle))\n",
        "\n",
        "#   Originally:  I read it in  The  Times of London\n",
        "# Should print:  i read it in the times of london   (with no stopword removal)\n",
        "# Should print:  read times london  (with stopword removal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU1qq3_SZBx_"
      },
      "source": [
        "# Variables for reading and preparing the text\n",
        "#\n",
        "# One article is stored as the text of a whole BBC article as a single string.\n",
        "#\n",
        "articles = []           # List of articles in original form.\n",
        "articlesPrepared = []   # Articles with texts prepared for tokenization\n",
        "#\n",
        "#  Each article has a label like \"business\", \"sport\", \"tech\" etc.\n",
        "labels = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eutB2xMiZD0e"
      },
      "source": [
        "# The BBC data is in a CSV spreadsheet file, one article in each spreadsheet row.\n",
        "# The two spreadsheet column headings of interest are 'text' and 'category'\n",
        "#\n",
        "# Read the spreadsheet and make three lists, with one entry for each article.\n",
        "#\n",
        "with open(path_bbc, 'r') as csvfile:\n",
        "    csv_reader = csv.DictReader(csvfile)\n",
        "    for item in csv_reader:\n",
        "        articles.append(item['text'])  # Article text\n",
        "        articlesPrepared.append(prepareText(item['text'])) # Prepared version\n",
        "        labels.append(item['category'])  # Article category label\n",
        "\n",
        "# Print the label and the texts of the first few articles\n",
        "for i in range(3):\n",
        "  print(labels[i])\n",
        "  print(articles[i])\n",
        "  print(articlesPrepared[i])\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfdaWh06ZGe3"
      },
      "source": [
        "# Now split into training and validation sets\n",
        "# Use the training_portion variable.\n",
        "#\n",
        "train_size =  # YOUR CODE HERE\n",
        "\n",
        "train_articles = # YOUR CODE HERE\n",
        "train_labels = # YOUR CODE HERE\n",
        "\n",
        "validation_articles =  # YOUR CODE HERE\n",
        "validation_labels =  # YOUR CODE HERE\n",
        "\n",
        "print(train_size)\n",
        "print(len(train_articles))\n",
        "print(len(train_labels))\n",
        "print(len(validation_articles))\n",
        "print(len(validation_labels))\n",
        "\n",
        "# Expected output (if training_portion=0.8)\n",
        "# 1780\n",
        "# 1780\n",
        "# 1780\n",
        "# 445\n",
        "# 445"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzA8xhwZI22"
      },
      "source": [
        "# The training articles: tokenize and turn into sequences of word-tokens\n",
        "#\n",
        "# Make a tokenizer object.\n",
        "#   It will utilize the vocab_size most common words\n",
        "#   The less common words will be repleaced with the oov_tok fake word\n",
        "#\n",
        "tokenizer =  # YOUR CODE HERE\n",
        "#\n",
        "#  Use your training articles to 'fit' the tokenizer. It will count all the words,\n",
        "#  assign numbers to the most common words.\n",
        "#\n",
        "tokenizer.   # YOUR CODE HERE\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert your training articles to sequences of number-tokens.\n",
        "# Then standardize the length of the sequences, truncating or padding as needed\n",
        "train_sequences =  # YOUR CODE HERE\n",
        "train_padded = # YOUR CODE HERE\n",
        "\n",
        "print(len(train_sequences[0]))\n",
        "\n",
        "print(len(train_padded[0]))\n",
        "\n",
        "print(len(train_sequences[1]))\n",
        "print(len(train_padded[1]))\n",
        "\n",
        "print(len(train_sequences[10]))\n",
        "print(len(train_padded[10]))\n",
        "\n",
        "# Expected Ouput (assuming training_portion = 0.8)\n",
        "# 449\n",
        "# 120\n",
        "# 200\n",
        "# 120\n",
        "# 192\n",
        "# 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8PeFWzPZLW_"
      },
      "source": [
        "# Now do the same for the validation articles: tokenize and turn into sequences\n",
        "#\n",
        "validation_sequences =  # YOUR CODE HERE\n",
        "validation_padded =  # YOUR CODE HERE\n",
        "\n",
        "print(len(validation_sequences))\n",
        "print(validation_padded.shape)\n",
        "\n",
        "# Expected output\n",
        "# 445\n",
        "# (445, 120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Xc-uWxXhML"
      },
      "source": [
        "#  Here you can decode and print the sequences back into article texts.\n",
        "#  Try it for a few sentences.\n",
        "#\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_sentence(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(train_articles[0])   # Try several different articles, not only 0\n",
        "print(decode_sentence(train_sequences[0]))\n",
        "\n",
        "#expected decoded ouput, assuming stopword removal\n",
        "# tv future hands viewers home theatre systems plasma high-definition tvs digital video\n",
        "# tv future <oov>  <oov>  home theatre systems <oov>  high <oov>  <oov>  digital video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkWiQ_FKZNp2"
      },
      "source": [
        "#  Here we convert the category words to numbers.\n",
        "#  \"sport\" will become 1, \"business\" will become 2, etc.\n",
        "#\n",
        "#  We can use the tokenizer as a kind of programming trick.\n",
        "#  We will 'fit' the tokenizer on the labels, each label is effectively a one-word sentence.\n",
        "#  So each category word will be assigned a number-token, the label will become a single number.\n",
        "#\n",
        "# First make a Tokenizer object and fit it.\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "print(label_tokenizer.word_index)\n",
        "# Should print:\n",
        "#   {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
        "\n",
        "# Convert the training and validation labels into number-tokens.\n",
        "# Then use np.array() to make 1-dimensional np arrays\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "\n",
        "print(training_label_seq[0])\n",
        "print(training_label_seq[1])\n",
        "print(training_label_seq[2])\n",
        "print(training_label_seq.shape)\n",
        "\n",
        "print(validation_label_seq[0])\n",
        "print(validation_label_seq[1])\n",
        "print(validation_label_seq[2])\n",
        "print(validation_label_seq.shape)\n",
        "\n",
        "# Expected output\n",
        "# [4]\n",
        "# [2]\n",
        "# [1]\n",
        "# (1780, 1)\n",
        "# [5]\n",
        "# [4]\n",
        "# [3]\n",
        "# (445, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ5um4MWZP-W"
      },
      "source": [
        "# Neural network!  Four layers.\n",
        "#\n",
        "#  First is an Embedding layer, which converts each number-token into a vector\n",
        "#  The output of the embedding layer is one vector for each word, so if each\n",
        "#  input sequence has 50 words (50 numbers-tokens) and the embedding dimension is 15,\n",
        "#  the output is a 2D tensor 50 x 15.\n",
        "#\n",
        "#  The next layer will either:\n",
        "#    a) Flatten the 2D tensor to 1-dimension, e.g. 750 numbers\n",
        "#    b) Average up the each dimension using GlobalAveragePooling1D.\n",
        "#       This will produce single vector representing an\n",
        "#       embedding of the average word, e.g. 15 numbers\n",
        "#\n",
        "#  Then comes a hidden layer, densely connected.\n",
        "#\n",
        "#  Finally a softmax output layer, one neuron for each label/category number.\n",
        "#  Output layer is six instead of five, as we are not using category number 0.\n",
        "#\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    #tf.keras.layers.GlobalAveragePooling1D(),    # YOUR CODE HERE: uncomment to\n",
        "    #tf.keras.layers.Flatten(),                   #  pick Flatten() or GlobalAveragePooling1D()\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Expected summary output with:\n",
        "#    120-word sequences and 16-number embedding vectors\n",
        "#    using GlobalAveragePooling for the 2nd layer\n",
        "#\n",
        "# Layer (type)                 Output Shape              Param #\n",
        "# =================================================================\n",
        "# embedding (Embedding)        (None, 120, 16)           16000\n",
        "# _________________________________________________________________\n",
        "# global_average_pooling1d (Gl (None, 16)                0\n",
        "# _________________________________________________________________\n",
        "# dense (Dense)                (None, 24)                408\n",
        "# _________________________________________________________________\n",
        "# dense_1 (Dense)              (None, 6)                 150\n",
        "# =================================================================\n",
        "# Total params: 16,558\n",
        "# Trainable params: 16,558\n",
        "# Non-trainable params: 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsfdxySKZSXu",
        "scrolled": true
      },
      "source": [
        "## Time to train and validate the neural network classifier, using model.fit()\n",
        "#\n",
        "# The validation data is tested after each training epoch. (We don't wait until training is done in order to test.)\n",
        "#\n",
        "num_epochs = 25\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ0BX2apXS9u"
      },
      "source": [
        "# These two graphs will show how the training and validation improved for each epoch.\n",
        "# It may show that the training data accuracy improves even after the validation data\n",
        "#  accuracy stopped improving.\n",
        "#\n",
        "#  This shows that the network is over-training a little bit.\n",
        "#\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhnFA_TDXrih"
      },
      "source": [
        "# For fun, if you are interested, you can extract the embedding vectors.\n",
        "# Here is how to extract them from the trained neural network.\n",
        "#\n",
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
        "\n",
        "# Expected output, using a maximum vocabulary of 1000 words and 16 element vectors\n",
        "# (1000, 16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will compare the embeddings of several words of your choice**, using cosine similarity.\n",
        "\n",
        "Since this network was trained for a classification task using only 5 categories, generally the words which are indicative of a particular category should show vector similarity."
      ],
      "metadata": {
        "id": "8MBy-KhxBZ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST: Put your words here and test them\n",
        "\n",
        "words = ['word1', 'word2', 'word3', 'word4', 'word5', 'word6'] # put your words here\n",
        "words = ['london', 'people', 'digital', 'lost', 'rugby', 'player'] # put your words here\n",
        "for w in words:\n",
        "  if not w in word_index:\n",
        "    print(w, \"not recognized\")\n",
        "  elif word_index[w] >= vocab_size:\n",
        "    print(w, word_index[w], \"> vocab_size\")\n",
        "  else:\n",
        "    print(w, word_index[w], 'OK')\n"
      ],
      "metadata": {
        "id": "-1fa0F4vZkc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECOND: Compute all comparisons\n",
        "\n",
        "# Compare two vectors with cosine: (A dot B) / (|A| * |B|)\n",
        "#\n",
        "def comp(A, B):\n",
        "  return np.dot(A,B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
        "\n",
        "# Print all comparisons\n",
        "for i in range(len(words)-1):\n",
        "  w1 = words[i]\n",
        "  v1 = weights[word_index[w1]]\n",
        "  for j in range(i+1,len(words)):\n",
        "    w2 = words[j]\n",
        "    v2 = weights[word_index[w2]]\n",
        "    print(\"cosine\", w1, w2, '=', comp(v1, v2))"
      ],
      "metadata": {
        "id": "Dkp_c4eNCx0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}