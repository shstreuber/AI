{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/AI/blob/main/Week_13_CS_345_545_Translator_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qJkPl_Enh1S"
      },
      "source": [
        "# **WELCOME TO THE TRANSLATION OPTIONS NOTEBOOK!**\n",
        "\n",
        "In this notebook, we explore different types of translation models and techniques for translating text from one language to another. Below are the items covered in this notebook:\n",
        "\n",
        "0. Recap of the Simple Transformer Model\n",
        "\n",
        "A. English to German Translation using LSTM\n",
        "\n",
        "B. English to German Translation using Custom Transformer Model\n",
        "\n",
        "C. English to German Translation using a Prebuilt Transformer Model\n",
        "\n",
        "Each section of the notebook demonstrates how to implement a translation model for a specific language pair using different architectures and approaches. By exploring these options, you'll gain insights into the strengths and limitations of each translation method and how to choose the most suitable approach for your translation task. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVMKAvz0nh1T"
      },
      "source": [
        "#**0. The Simple Transformer Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpuE1U6inh1U"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class Transformer(Model):\n",
        "    def __init__(self, vocab_size, max_sequence_length, d_model, num_heads, num_layers, dropout_rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Define embedding layer\n",
        "        self.embedding_layer = Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Define positional encoding layer\n",
        "        self.positional_encoding = self.get_positional_encoding(max_sequence_length, d_model)\n",
        "\n",
        "        # Define transformer layers\n",
        "        self.transformer_layers = [self.create_transformer_layer(d_model, num_heads, dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "        # Define output layer\n",
        "        self.output_layer = Dense(vocab_size)\n",
        "\n",
        "    def get_positional_encoding(self, max_sequence_length, d_model):\n",
        "        # Calculate positional encodings\n",
        "        positional_encoding = []\n",
        "        for pos in range(max_sequence_length):\n",
        "            pos_encoding = [pos / tf.pow(tf.constant(10000, dtype=tf.float32), 2 * (i // 2) / tf.cast(d_model, tf.float32)) for i in range(d_model)]\n",
        "            if pos % 2 == 0:\n",
        "                positional_encoding.append(tf.math.sin(pos_encoding))\n",
        "            else:\n",
        "                positional_encoding.append(tf.math.cos(pos_encoding))\n",
        "        positional_encoding = tf.stack(positional_encoding)\n",
        "        return tf.expand_dims(positional_encoding, axis=0)\n",
        "\n",
        "    def create_transformer_layer(self, d_model, num_heads, dropout_rate):\n",
        "        # Create transformer layer\n",
        "        return MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads, dropout=dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Define forward pass of the model\n",
        "        input_sequence, target_sequence = inputs\n",
        "\n",
        "        # Embed input sequence and add positional encoding\n",
        "        embedded_input = self.embedding_layer(input_sequence) + self.positional_encoding[:, :tf.shape(input_sequence)[1], :]\n",
        "\n",
        "        # Apply transformer layers sequentially\n",
        "        transformer_output = embedded_input\n",
        "        for layer in self.transformer_layers:\n",
        "            transformer_output = layer(query=transformer_output, value=transformer_output, attention_mask=None, training=training)\n",
        "\n",
        "        # Apply output layer\n",
        "        output_logits = self.output_layer(transformer_output)\n",
        "\n",
        "        return output_logits\n",
        "\n",
        "# Example usage:\n",
        "vocab_size = 10000  # Example vocabulary size\n",
        "max_sequence_length = 50  # Example maximum sequence length\n",
        "d_model = 128  # Example model dimensionality\n",
        "num_heads = 4  # Example number of attention heads\n",
        "num_layers = 2  # Example number of transformer layers\n",
        "dropout_rate = 0.1  # Example dropout rate\n",
        "\n",
        "# Instantiate the Transformer model\n",
        "transformer_model = Transformer(vocab_size, max_sequence_length, d_model, num_heads, num_layers, dropout_rate)\n",
        "\n",
        "# Compile the model\n",
        "transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Define and initialize tokenizer object (replace this with your actual tokenizer)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "\n",
        "# Example usage with text data\n",
        "input_text = [\"This is an example sentence\", \"Another example sentence\"]\n",
        "target_text = [\"Dies ist ein Beispiel Satz\", \"Ein weiterer Beispiel Satz\"]\n",
        "\n",
        "# Tokenize input sequences\n",
        "input_sequences = tokenizer.texts_to_sequences(input_text)\n",
        "\n",
        "# Pad sequences to ensure equal length\n",
        "input_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Tokenize target sequences (if applicable)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_text)\n",
        "target_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Define model inputs\n",
        "inputs = (input_sequences_padded, target_sequences_padded)\n",
        "\n",
        "# Feed data into the model\n",
        "output_logits = transformer_model(inputs)\n",
        "\n",
        "# Extract predictions (if applicable)\n",
        "predicted_sequences = tf.argmax(output_logits, axis=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "700fLeIjnh1U"
      },
      "source": [
        "#**A. Translator with LSTM**\n",
        "In this completed code:\n",
        "\n",
        "* English and German sentences are tokenized using the Tokenizer class.\n",
        "* The sequences are padded to ensure equal length using the pad_sequences function.\n",
        "* The model architecture is defined with an embedding layer, LSTM layers for both the encoder and decoder, and a dense layer for the decoder output.\n",
        "* The model is compiled with the RMSprop optimizer and sparse categorical cross-entropy loss.\n",
        "* Finally, the model is trained using the fit method with the English and German sequences as input and target, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXc6Z9Bbnh1U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Add start/end tokens to target sentences\n",
        "english_sentences = ['I am a student', 'He likes apples', 'She is reading a book']\n",
        "german_sentences = ['<start> Ich bin ein Student <end>',\n",
        "                    '<start> Er mag Ã„pfel <end>',\n",
        "                    '<start> Sie liest ein Buch <end>']\n",
        "\n",
        "# Tokenize the English sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "max_english_length = max(len(seq) for seq in english_sequences)\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "\n",
        "# Tokenize the German sentences\n",
        "german_tokenizer = Tokenizer()\n",
        "german_tokenizer.fit_on_texts(german_sentences)\n",
        "german_sequences = german_tokenizer.texts_to_sequences(german_sentences)\n",
        "max_german_length = max(len(seq) for seq in german_sequences)\n",
        "german_vocab_size = len(german_tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences\n",
        "english_sequences_padded = pad_sequences(english_sequences, maxlen=max_english_length, padding='post')\n",
        "german_sequences_padded = pad_sequences(german_sequences, maxlen=max_german_length, padding='post')\n",
        "\n",
        "# Define model architecture\n",
        "latent_dim = 256\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=english_vocab_size, output_dim=latent_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(input_dim=german_vocab_size, output_dim=latent_dim, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(german_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define and compile the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    [english_sequences_padded, german_sequences_padded[:, :-1]],\n",
        "    np.expand_dims(german_sequences_padded[:, 1:], -1),\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    validation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DOL-kz_nh1U"
      },
      "outputs": [],
      "source": [
        "# === Define Inference Models ===\n",
        "\n",
        "# Define decoder embedding layer separately so it can be reused later\n",
        "decoder_embedding_layer = Embedding(input_dim=german_vocab_size, output_dim=latent_dim, mask_zero=True)\n",
        "\n",
        "# Encoder model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder model for inference\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_embedding_output = decoder_embedding_layer(decoder_inputs_single)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_output, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# === Translation / Decoding ===\n",
        "\n",
        "# Example English sentences to translate\n",
        "english_test_sentences = ['We are happy', 'They love ice cream']\n",
        "\n",
        "# Tokenize and pad test data\n",
        "english_test_sequences = english_tokenizer.texts_to_sequences(english_test_sentences)\n",
        "english_test_sequences_padded = pad_sequences(english_test_sequences, maxlen=max_english_length, padding='post')\n",
        "\n",
        "# Function to decode predicted sequences\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Start with the <start> token\n",
        "    start_token_index = german_tokenizer.word_index.get('<start>', 1)\n",
        "    end_token_index = german_tokenizer.word_index.get('<end>', 2)\n",
        "    target_seq = np.array([[start_token_index]])\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = german_tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_token_index == end_token_index or len(decoded_sentence.split()) > max_german_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += sampled_word + ' '\n",
        "\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# Predict and decode German translations\n",
        "predicted_sentences = []\n",
        "for seq in english_test_sequences_padded:\n",
        "    decoded = decode_sequence(seq.reshape(1, -1))\n",
        "    predicted_sentences.append(decoded)\n",
        "\n",
        "# Print the translations\n",
        "for english_sentence, german_translation in zip(english_test_sentences, predicted_sentences):\n",
        "    print('English:', english_sentence)\n",
        "    print('German:', german_translation)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ISMg7Xnh1U"
      },
      "source": [
        "#**B. Translator with Transformer: Option 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhjJNHsLnh1V"
      },
      "outputs": [],
      "source": [
        "# Import the needed libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCzi4BObnh1V"
      },
      "source": [
        "##**1. Encoder Layer**\n",
        "The class TransformerEncoderLayer defines a single layer within the encoder of a Transformer model. In a Transformer architecture, the encoder is responsible for processing input sequences and extracting their representations. Each layer in the encoder consists of two main components: a multi-head self-attention mechanism and a position-wise feedforward neural network (FFNN).\n",
        "\n",
        "Here's a breakdown of what the class does:\n",
        "\n",
        "1. Initialization: The constructor initializes the layer, taking parameters such as the dimensionality of the model (d_model), the number of attention heads (num_heads), the dimensionality of the feedforward neural network (dff), and the dropout rate (rate).\n",
        "\n",
        "2. Components: Inside the layer, there are three main components:\n",
        "\n",
        "* Multi-Head Self-Attention (MHA): This component allows the model to focus on different parts of the input sequence simultaneously. It takes the input sequence, computes attention weights for each position, and combines information from different positions based on these weights.\n",
        "* Feedforward Neural Network (FFNN): After the attention mechanism, the output passes through a position-wise feedforward neural network. This network applies a series of linear transformations and non-linear activations to each position independently.\n",
        "* Layer Normalization and Dropout: Layer normalization and dropout are applied after each sub-layer (attention and feedforward network) to stabilize training and prevent overfitting.\n",
        "3. Call Method: The call method defines how the layer processes input data. It takes the input sequence and a boolean flag indicating whether the model is in training mode. Inside the method, the input sequence is passed through the multi-head self-attention mechanism and feedforward neural network in sequence. Layer normalization and dropout are applied after each sub-layer, and the output of the layer is returned.\n",
        "\n",
        "In summary, the TransformerEncoderLayer class encapsulates the functionality of a single layer within the encoder of a Transformer model. It performs operations such as multi-head self-attention and position-wise feedforward processing on input sequences, facilitating the extraction of meaningful representations from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4COYz6Rinh1V"
      },
      "outputs": [],
      "source": [
        "# Define the TransformerEncoderLayer and TransformerDecoderLayer classes\n",
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        # Feedforward neural network\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Self-attention mechanism\n",
        "        attn_output = self.mha(inputs, inputs, attention_mask=None, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Feedforward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV0aIlrnnh1V"
      },
      "source": [
        "##**2. Decoder Layer**\n",
        "The TransformerDecoderLayer class defines a single layer within the decoder of a Transformer model. In the Transformer architecture, the decoder is responsible for generating output sequences based on the representations learned by the encoder. Each layer in the decoder consists of three main components: multi-head self-attention, encoder-decoder attention, and a position-wise feedforward neural network (FFNN).\n",
        "\n",
        "Here's an explanation of what the class does:\n",
        "\n",
        "1. Initialization: The constructor initializes the layer, taking parameters such as the dimensionality of the model (d_model), the number of attention heads (num_heads), the dimensionality of the feedforward neural network (dff), and the dropout rate (rate).\n",
        "\n",
        "2. Components: Inside the layer, there are three main components:\n",
        "\n",
        "* Masked Multi-Head Self-Attention (MHA): This component allows the decoder to attend to previous positions in the output sequence while preventing it from attending to future positions. It computes attention weights for each position in the output sequence and combines information from different positions based on these weights.\n",
        "* Encoder-Decoder Attention: This component allows the decoder to take into account the representations learned by the encoder. It computes attention weights between the current position in the output sequence and the input sequence representations, enabling the decoder to align its output with the input.\n",
        "* Feedforward Neural Network (FFNN): After the attention mechanisms, the output passes through a position-wise feedforward neural network, similar to the encoder. This network applies a series of linear transformations and non-linear activations to each position independently.\n",
        "* Layer Normalization and Dropout: Layer normalization and dropout are applied after each sub-layer (masked self-attention, encoder-decoder attention, and feedforward network) to stabilize training and prevent overfitting.\n",
        "3. Call Method: The call method defines how the layer processes input data. It takes the input sequence, encoder output, and a boolean flag indicating whether the model is in training mode. Inside the method, the input sequence is passed through the masked multi-head self-attention mechanism, encoder-decoder attention mechanism, and feedforward neural network in sequence. Layer normalization and dropout are applied after each sub-layer, and the output of the layer is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDRz2Zyhnh1V"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head attention mechanisms\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "\n",
        "        # Feedforward neural network\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        # Self-attention mechanism\n",
        "        attn1 = self.mha1(inputs, inputs, attention_mask=None, training=training)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + inputs)\n",
        "\n",
        "        # Encoder-decoder attention mechanism\n",
        "        attn2 = self.mha2(out1, enc_output, attention_mask=None, training=training)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        # Feedforward network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucaXNw0jnh1V"
      },
      "source": [
        "##**3. Implement the Translator**\n",
        "The TransformerTranslator class is a custom Keras model that implements a sequence-to-sequence transformer for translation tasks, such as English to German translation. Here's what the class does:\n",
        "\n",
        "1. Initialization: The constructor initializes the model, taking parameters such as the number of layers (num_layers), the dimensionality of the model (d_model), the number of attention heads (num_heads), the dimensionality of the feedforward neural network (dff), the vocabulary size of the input and target languages (input_vocab_size, target_vocab_size), and the dropout rate (dropout_rate).\n",
        "\n",
        "2. Components: Inside the model, there are two main components:\n",
        "\n",
        "* Encoder: The encoder consists of a stack of transformer encoder layers. Each layer contains multi-head self-attention and feedforward neural network sub-layers. The encoder processes the input sequences (English sentences) and generates representations for each token in the input sequence.\n",
        "* Decoder: The decoder consists of a stack of transformer decoder layers. Each layer contains masked multi-head self-attention, encoder-decoder attention, and feedforward neural network sub-layers. The decoder takes the encoder output and generates output sequences (German translations) based on the learned representations.\n",
        "3. Forward Pass: The call method defines the forward pass of the model. Given input sequences in both the source (English) and target (German) languages, the model passes the input through the encoder to generate encoder output representations. The decoder then takes these representations and generates output sequences in the target language. The final layer of the decoder outputs the predicted token probabilities for each position in the output sequence.\n",
        "4. Loss Calculation: During training, the model calculates the loss between the predicted token probabilities and the actual target tokens using a loss function such as sparse categorical cross-entropy. This loss is used to update the model's weights during optimization.\n",
        "5. Model Compilation: After defining the forward pass and loss calculation, the model is compiled using an optimizer (e.g., Adam) and a loss function. This step prepares the model for training.\n",
        "6. Model Summary: Finally, the model's architecture is summarized using the summary method, which displays the layer names, output shapes, and number of parameters in the model.\n",
        "\n",
        "In summary, the TransformerTranslator class encapsulates the functionality of a sequence-to-sequence transformer model for translation tasks. It combines encoder and decoder components to process input sequences and generate output sequences, facilitating tasks such as language translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHbyZSpFnh1V"
      },
      "outputs": [],
      "source": [
        "# Define the TransformerTranslator model\n",
        "class TransformerTranslator(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "        super(TransformerTranslator, self).__init__()\n",
        "\n",
        "        # Embedding layers for input and target vocabularies\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "\n",
        "        # Transformer encoder and decoder layers\n",
        "        self.transformer_encoder_layers = [TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                                           for _ in range(num_layers)]\n",
        "        self.transformer_decoder_layers = [TransformerDecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                                           for _ in range(num_layers)]\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, targets, training=False):\n",
        "        # Encoder padding mask\n",
        "        enc_padding_mask = None\n",
        "        # Decoder padding mask\n",
        "        dec_padding_mask = None\n",
        "\n",
        "        # Encoder\n",
        "        encoder_input = self.encoder_embedding(inputs)\n",
        "        for layer in self.transformer_encoder_layers:\n",
        "            encoder_input = layer(encoder_input, training=training)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = self.decoder_embedding(targets)\n",
        "        for layer in self.transformer_decoder_layers:\n",
        "            decoder_input = layer(decoder_input, encoder_input, training=training)\n",
        "\n",
        "        # Final output\n",
        "        output = self.final_layer(decoder_input)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEorunO6nh1V"
      },
      "source": [
        "##**4. Configure Transformer Architecture**\n",
        "The example hyperparameters define various aspects of the transformer architecture and training process. Here's a breakdown of what each hyperparameter does:\n",
        "\n",
        "* num_layers: This hyperparameter determines the number of encoder and decoder layers in the transformer model. Each layer contains multiple sub-layers, such as multi-head self-attention and feedforward neural networks.\n",
        "\n",
        "* d_model: The dimensionality of the model, also known as the hidden size. It represents the dimensionality of the embedding space and the internal representations within the transformer layers.\n",
        "\n",
        "* num_heads: The number of attention heads in the multi-head attention mechanism. More attention heads allow the model to focus on different parts of the input sequence simultaneously, capturing more complex relationships.\n",
        "\n",
        "* dff: The dimensionality of the feedforward neural network layer within each transformer layer. It determines the size of the hidden layer in the feedforward network.\n",
        "\n",
        "* input_vocab_size: The size of the vocabulary for the input language (e.g., English). It represents the total number of unique tokens or words in the vocabulary.\n",
        "\n",
        "* target_vocab_size: The size of the vocabulary for the target language (e.g., German). Similar to the input vocabulary size, it represents the total number of unique tokens or words in the target language vocabulary.\n",
        "\n",
        "* dropout_rate: The dropout rate applied to the outputs of each transformer layer during training. Dropout is a regularization technique that helps prevent overfitting by randomly dropping units (along with their connections) from the network during training.\n",
        "\n",
        "These hyperparameters collectively define the architecture and behavior of the transformer model, influencing its capacity, attention mechanism, and regularization during training. Adjusting these hyperparameters allows practitioners to tailor the model to specific tasks and datasets, balancing model complexity and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJqBgP8Lnh1V"
      },
      "outputs": [],
      "source": [
        "# Example hyperparameters\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 10000\n",
        "target_vocab_size = 10000\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o99X53mZnh1V"
      },
      "source": [
        "##**5. Initialize and Compile the Model**\n",
        "The \"Initialize and compile the model\" section of the code involves setting up the model architecture and configuring its training process. Here's what each step in this section does:\n",
        "\n",
        "* Model Initialization: The TransformerTranslator class is instantiated with the specified hyperparameters (num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate). This creates an instance of the transformer model with the specified architecture.\n",
        "\n",
        "* Optimizer Initialization: An optimizer is chosen to update the model parameters during training. In this case, the Adam optimizer is used, which is a popular optimization algorithm for deep learning models. It adapts the learning rate for each parameter during training.\n",
        "\n",
        "* Loss Function Initialization: The loss function is defined to measure the discrepancy between the model predictions and the actual target values. In this example, SparseCategoricalCrossentropy is used as the loss function. It is suitable for classification tasks with integer targets and sparse target values.\n",
        "\n",
        "* Model Compilation: The model is compiled with the optimizer and loss function. This step configures the model for training by specifying the optimization algorithm and the loss function to be minimized. Additionally, any additional metrics can be specified here to monitor during training.\n",
        "\n",
        "Overall, this section prepares the model for training by specifying its architecture, optimization algorithm, loss function, and any additional metrics to be tracked. After compilation, the model is ready to be trained on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFm4ijtpnh1V"
      },
      "outputs": [],
      "source": [
        "# Initialize and compile the model\n",
        "model = TransformerTranslator(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate)\n",
        "optimizer = Adam()\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Define custom loss function\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFSqEfMunh1V"
      },
      "source": [
        "##**6. Model Summary**\n",
        "The \"Display model summary\" section of the code generates a summary of the model architecture, providing useful information about the different layers and parameters in the model. Here's what each step in this section does:\n",
        "\n",
        "1. Model Summary: The summary() method is called on the model object. This method prints a concise summary of the model architecture to the console. It includes information such as:\n",
        "* The type of each layer in the model.\n",
        "* The output shape of each layer.\n",
        "* The number of parameters in each layer.\n",
        "* The total number of parameters in the model.\n",
        "By displaying the model summary, you can quickly inspect the architecture of the model, including the number of layers, the shape of the input and output tensors, and the number of trainable parameters. This information is helpful for debugging, optimizing the model, and ensuring that the architecture is as intended.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDHZQ8vynh1V"
      },
      "outputs": [],
      "source": [
        "# Display model summary\n",
        "\n",
        "# Generate random example inputs and outputs for the model\n",
        "# The input is a batch of sequences (64 sequences of length 50) for the encoder\n",
        "input_example = tf.random.uniform((64, 50), minval=0, maxval=input_vocab_size, dtype=tf.int32)\n",
        "\n",
        "# The output is a batch of sequences (64 sequences of length 50) for the decoder\n",
        "output_example = tf.random.uniform((64, 50), minval=0, maxval=target_vocab_size, dtype=tf.int32)\n",
        "\n",
        "# Create padding masks for both encoder and decoder\n",
        "# Here we assume that padding tokens are represented by 0\n",
        "# The encoder_padding_mask is True (1) wherever input tokens are padding (0)\n",
        "encoder_padding_mask = tf.cast(tf.math.equal(input_example, 0), tf.float32)\n",
        "\n",
        "# The decoder_padding_mask is True (1) wherever output tokens are padding (0)\n",
        "decoder_padding_mask = tf.cast(tf.math.equal(output_example, 0), tf.float32)\n",
        "\n",
        "# Run the model with the generated input and output examples\n",
        "# Pass 'training=False' as a keyword argument to avoid the positional argument error\n",
        "model(input_example, output_example, training=False)\n",
        "\n",
        "# Display the model architecture and parameters\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lqNKHC2nh1V"
      },
      "source": [
        "##**THE ENTIRE CODE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5P6t006nh1W"
      },
      "outputs": [],
      "source": [
        "# ****************** THE ENTIRE CODE *******************\n",
        "# Import the needed libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import numpy as np\n",
        "\n",
        "# Define the TransformerEncoderLayer class\n",
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        # Feedforward neural network\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Self-attention mechanism\n",
        "        attn_output = self.mha(inputs, inputs, attention_mask=None, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Feedforward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "# Define the TransformerDecoderLayer class\n",
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head attention mechanisms\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "\n",
        "        # Feedforward neural network\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        # Self-attention mechanism\n",
        "        attn1 = self.mha1(inputs, inputs, attention_mask=None, training=training)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + inputs)\n",
        "\n",
        "        # Encoder-decoder attention mechanism\n",
        "        attn2 = self.mha2(out1, enc_output, attention_mask=None, training=training)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        # Feedforward network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3\n",
        "\n",
        "\n",
        "# Define the TransformerTranslator model\n",
        "class TransformerTranslator(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "        super(TransformerTranslator, self).__init__()\n",
        "\n",
        "        # Embedding layers for input and target vocabularies\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "\n",
        "        # Transformer encoder and decoder layers\n",
        "        self.transformer_encoder_layers = [TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                                           for _ in range(num_layers)]\n",
        "        self.transformer_decoder_layers = [TransformerDecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                                           for _ in range(num_layers)]\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, targets, training=False):\n",
        "        # Encoder padding mask\n",
        "        enc_padding_mask = None\n",
        "        # Decoder padding mask\n",
        "        dec_padding_mask = None\n",
        "\n",
        "        # Encoder\n",
        "        encoder_input = self.encoder_embedding(inputs)\n",
        "        for layer in self.transformer_encoder_layers:\n",
        "            encoder_input = layer(encoder_input, training=training)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = self.decoder_embedding(targets)\n",
        "        for layer in self.transformer_decoder_layers:\n",
        "            decoder_input = layer(decoder_input, encoder_input, training=training)\n",
        "\n",
        "        # Final output\n",
        "        output = self.final_layer(decoder_input)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Example hyperparameters\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 10000\n",
        "target_vocab_size = 10000\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Initialize and compile the model\n",
        "model = TransformerTranslator(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate)\n",
        "optimizer = Adam()\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Define custom loss function\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function)\n",
        "\n",
        "# Display model summary\n",
        "input_example = tf.random.uniform((64, 50), minval=0, maxval=input_vocab_size, dtype=tf.int32)\n",
        "output_example = tf.random.uniform((64, 50), minval=0, maxval=target_vocab_size, dtype=tf.int32)\n",
        "\n",
        "# Run the model with the generated input and output examples\n",
        "# Pass 'training=False' as a keyword argument to avoid the positional argument error\n",
        "model(input_example, output_example, training=False)\n",
        "\n",
        "# Display the model architecture and parameters\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K58wHSV0nh1W"
      },
      "source": [
        "#**C. Translator with Transformer: Option 2**\n",
        "\n",
        "The code below uses the transformers library to load a pre-trained translation model (**opus-mt-en-de**) and its tokenizer. Then, it translates English sentences to German using the model's generate method. Finally, it prints the translations.\n",
        "\n",
        "The **opus-mt-en-de** library is a machine translation (MT) model trained specifically for translating text from English to German. It utilizes the OPUS-MT framework, which is an open-source initiative that provides pre-trained neural machine translation models for various language pairs. The opus-mt-en-de model is trained on a large corpus of English-German parallel text data and is capable of translating English sentences into German with high accuracy. This library makes it easy to perform English to German translation tasks programmatically by providing a ready-to-use translation model.\n",
        "\n",
        "Please make sure you have the transformers library installed (pip install transformers) before running this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM8RzV2Jnh1W"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Translate English sentences to German\n",
        "english_sentences = ['I am a student', 'He likes apples', 'She is reading a book']\n",
        "german_translations = []\n",
        "\n",
        "for sentence in english_sentences:\n",
        "    input_ids = tokenizer.encode(sentence, return_tensors=\"tf\", padding=True)\n",
        "    output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "    german_translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    german_translations.append(german_translation)\n",
        "\n",
        "# Print translations\n",
        "for english_sentence, german_translation in zip(english_sentences, german_translations):\n",
        "    print('English:', english_sentence)\n",
        "    print('German:', german_translation)\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}