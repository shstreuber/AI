{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYlB1rH0ljcrs5fl7WslTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shstreuber/AI/blob/main/Week4_TotalInsurance_VehicleDivision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Job**\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/totalinsurance.jpg\" width=\"300\">\n",
        "</div>\n",
        "\n",
        "**CONGRATULATIONS AND WELCOME TO THE TOTAL INSURANCE VEHICLE DIVISION!**\n",
        "</div>\n",
        "\n",
        "Your managers at Total Insurance, an insurance carrier for home, health, and vehicles, have seen your work in their Health Division and are suitably impressed. You have received 1,000 mini balloons to decorate your desk.\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://github.com/shstreuber/Data-Mining/blob/master/images/1000.JPG?raw=true\" width=\"300\">\n",
        "</div>\n",
        "</center>\n",
        "\n",
        "Total Insurance has had significant losses from car insurance fraud, i.e. clients reporting accidents that never happened or inflating the repair or replacement cost for their vehicles--and the company needs to crack down on this problem (so they can afford more mini balloons).\n",
        "\n",
        "You have been given an actual database export, and it is your job now to build a Deep Learning architecture that will predict\n",
        "\n",
        "1. Whether a client is likely to commit insurance fraud or not, or\n",
        "2. How many claims the client may submit to the insurance in the future.\n",
        "\n",
        "This is a large dataset with a lot of dimensions, which complicates your assignment. You will need to:\n",
        "1. Clean the data (your manager has already prepared that part)\n",
        "2. Determine your target attribute\n",
        "2. Determine which attributes/ features are most relevant to predict your target attribute.\n",
        "3. Preprocess the features\n",
        "4. Build the model\n",
        "4. Run and optimize the model and PROVE that it does what it is supposed to do."
      ],
      "metadata": {
        "id": "kQp8mcU16ynv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**0. Preparation**\n",
        "We will build our Deep Learning architecture on Tensorflow. Why Tensorflow? Because it is easier to build on Colab than Pytorch (for more about the battle of the giants, i.e. Tensorflow vs Pytorch, [read here](https://))."
      ],
      "metadata": {
        "id": "PAfeogrEUUcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyKMNeVHP5u4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # This tells Colab that we are using TensorFlow\n",
        "\n",
        "from tensorflow import keras # This is the main TensorFlow library\n",
        "from tensorflow.keras.models import Sequential # We are building a model that runs its layers in sequential order\n",
        "from tensorflow.keras import layers # We are building a Neural Network with several hidden layers\n",
        "from tensorflow.keras.layers import Dense # This will help us build a fully connected architecture\n",
        "from tensorflow.keras.layers import Normalization, Rescaling\n",
        "\n",
        "print(\"Current TensorFlow version is\", tf.__version__)\n",
        "\n",
        "import numpy as np # Your basic mathematical library for big datasets\n",
        "import pandas as pd # The library you need in order to clean and manipulate big datasets\n",
        "import matplotlib.pyplot as plt # Makes pretty pictures\n",
        "import seaborn as sns # for visualization aka more pretty pictures\n",
        "from sklearn.model_selection import train_test_split # Scikit-Learn is the default data science library\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42) # Setting a seed value for the randomizer so we get repeatable results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading in the data as fraud dataframe\n",
        "fraud = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/car_insurance_fraud.csv\")\n",
        "fraud.head(10) # Let's look at the first 10 rows"
      ],
      "metadata": {
        "id": "875KtF4YNOLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Exploratory data analysis involves techniques such as summary statistics, data visualization, and graphical representations to reveal hidden patterns, relationships, or trends within the data. EDA is an essential preliminary step in the data analysis pipeline, helping to uncover meaningful information and guide further exploration or hypothesis testing.\n",
        "\n",
        "You can get a quick and easy overview with df.describe(include = all). To identify missing values and datatypes, df.isna().sum() and df.dtypes work well.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vh0oeQbRNZNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Any missing values?\n",
        "fraud.isna().sum()"
      ],
      "metadata": {
        "id": "AxF_pGzl3vXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What data types and input features do we have? What could be our output label? Do we already have a label that contains the information, or do we need to create one?\n",
        "fraud.dtypes"
      ],
      "metadata": {
        "id": "UnEk4K33PQKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look more closely at the data\n",
        "fraud.describe(),print(\"***NUMERIC DATA OVERVIEW***\")# Build a data summary for the NUMERIC data in the set. Let's see if this will be enough"
      ],
      "metadata": {
        "id": "PavgjMIjQsIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look more closely at all data. Do we need all of these in order to predict whether someone has committed insurance fraud?\n",
        "fraud.describe(include = 'all'), print(\"***DATA OVERVIEW***\") # Build a data summary for ALL data in the set (not just numeric!)"
      ],
      "metadata": {
        "id": "tOjMCIp1NUm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Findings**\n",
        "0. **MISSING DATA?** No. All fields have 15420 data points.\n",
        "1. **POSSIBLE TARGET VARIABLES**:\n",
        "  \n",
        "    **- BINARY** (Classification): Potentially FraudFound_P, which is either 0 or 1. We could predict whether certain input parameters make it more or less likely that a claim is fraudulent.\n",
        "    \n",
        "    **- CATEGORICAL** (Classification): Potentially PastNumberOfClaims, which has 10 levels. We could predict how many claims a driver with certain parameters is likely to make.\n",
        "\n",
        "    **- NUMERIC** (Regression): We could predict the VehiclePrice, which, when paired with the BasePolicy, could determine the insurance payout if car is  total loss.\n",
        "2. **QUESTIONS TO ASK**: Which input attributes are relevant to our target variable? Which input attributes are not relevant?\n",
        "3. **ANY PROBLEMATIC DATA?**: Yes; DayOfWeekClaimed and MonthClaimed have 8 and 13 levels when they should have 7 and 12. To determine what to do, we will check how many datapoints these are; if not too many, their (non)existence will not affect the final outcome, and we can discard. If more than 5%, we will keep and substitute an appropriate value.\n",
        "4. **ANY INCONSISTENT DATA?**: Yes. The Age Variable contains a group of 0 (zero) values that either need to be cleaned up or the rows discarded. To determine what to do, we will check how many datapoints these are; if not too many, their (non)existence will not affect the final outcome, and we can discard. If more than 5%, we will keep and substitute an appropriate value based on a related measure, AgeOfPolicyHolder.\n",
        "5. **ANY OPPORTUNITIES FOR SIMPLIFYING?**: Yes. AgeOfVehicle should really be numeric and can be easily transformed.\n",
        "6. **ANY OPPORTUNITIES FOR REDUCING THE DATAFRAME FOR EASE OF PROCESSING IN A NEURAL NETWORK?**: Yes. We know that multiple layers are very computation-heavy, so smaller datasets process more easily. We will evaluate whether we need all the attributes to predict the chosen target variable."
      ],
      "metadata": {
        "id": "B3-NZ8gNSG_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2A. Data Cleanup**\n",
        "Let's clean the data up so we remove any potential noise and clean up any missing and inconsistent values. We will address findings 3 and 4 above.\n",
        "\n",
        "When dealing with text, data cleanup often also involves removing special characters or changing character sets."
      ],
      "metadata": {
        "id": "tAML7aPIbZWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2A1: DayOfWeekClaimed and MonthClaimed Issue**"
      ],
      "metadata": {
        "id": "Pn5fzqGmICCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 8 Days and 13 Months\n",
        "\n",
        "DOWC = fraud.DayOfWeekClaimed.unique()\n",
        "MC = fraud.MonthClaimed.unique()\n",
        "print(\"DayOfWeekClaimed:\", DOWC, \"\\nMonthClaimed:\", MC)"
      ],
      "metadata": {
        "id": "78R-LdB795Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many rows with '0' in DayOfWeekClaimed and MonthClaimed?\n",
        "DOWC0_count = fraud[fraud['DayOfWeekClaimed'] == '0']\n",
        "MC0_count = fraud[fraud['MonthClaimed'] == '0']\n",
        "# print(\"DayOfWeekClaimed:\", DOWC0_count, \"\\nMonthClaimed:\", MC0_count)\n",
        "DOWC0_count\n",
        "MC0_count"
      ],
      "metadata": {
        "id": "I2PV8_QC-TpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OK, just one row! Let's verify that the index number is 1516.\n",
        "display(fraud.iloc[1516])"
      ],
      "metadata": {
        "id": "XbhmaC6QE0_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get rid of that row! We know that the index number is 1516 and can use iloc to locate it in the dataframe.\n",
        "fraud.drop(1516, inplace = True)\n",
        "display(fraud.iloc[1516])\n",
        "\n",
        "# Note the name below of the new row with index 1516!"
      ],
      "metadata": {
        "id": "BgQzZ9NKB7mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2A2: Age Variable Issue**"
      ],
      "metadata": {
        "id": "7y5vYwzvIUuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. The Age variable containing the 0 values--how relevant is that? Let's check the distribution and the count:\n",
        "\n",
        "fraud.Age.hist() # Visually check whether Age == 0 is an outlier; if small outlier, then drop the rows?\n",
        "Age0_count = fraud[fraud['Age'] == 0].count() # How many rows are impacted?\n",
        "Age0_percent = (Age0_count/fraud.count())*100 # How many percent of rows are impacted? If less than 5%, we will drop the rows\n",
        "\n",
        "print('Rows with Age == 0:', Age0_count[1], '*** Percent of Total:', Age0_percent[1],\"%\")"
      ],
      "metadata": {
        "id": "Ark4FapzYZfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The 2% really doesn't make a difference, so we are removing them\n",
        "fraud = fraud[fraud['Age'] > 0]\n",
        "fraud.shape"
      ],
      "metadata": {
        "id": "0efSoB08n-b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2B. Preprocessing**\n",
        "Preprocessing means formatting the data such that it can work with the selected algorithm(s).\n",
        "\n",
        "This is the point at which you will start making decisions on how you want to design your model.\n",
        "<center>\n",
        "<img src = \"https://media.licdn.com/dms/image/C5612AQHWhI_0S0b_Pw/article-cover_image-shrink_600_2000/0/1645684670460?e=2147483647&v=beta&t=Nh2TVm1ND20u1G7BV8IHTqEw-ufvlntMqHNf14CZHzs\">\n",
        "</center>\n",
        "\n",
        "And some decisions will be easier than others."
      ],
      "metadata": {
        "id": "WJqw3Zukz4qU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2B1: Age of Vehicle Cleanup--Data Substitution**"
      ],
      "metadata": {
        "id": "j-daSmW8KqbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Age of Vehicle can be easily transformed into numbers\n",
        "fraud.AgeOfVehicle.unique() # How many levels do we have in this categorical attribute?"
      ],
      "metadata": {
        "id": "InmMHePCD0Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud.AgeOfVehicle.replace('new','1', inplace=True)\n",
        "fraud.AgeOfVehicle.replace('2 years','2', inplace=True)\n",
        "fraud.AgeOfVehicle.replace('3 years','3', inplace=True)\n",
        "fraud.AgeOfVehicle.replace('4 years','4', inplace=True)\n",
        "fraud.AgeOfVehicle.replace('5 years','5', inplace=True)\n",
        "fraud.AgeOfVehicle.replace('6 years','6', inplace=True)\n",
        "fraud.AgeOfVehicle.replace('7 years','7', inplace=True)\n",
        "fraud.AgeOfVehicle.replace('more than 7','10', inplace=True)\n",
        "fraud.AgeOfVehicle.head()"
      ],
      "metadata": {
        "id": "RwOJpxSsTjL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since data type is still object, we will make this numeric\n",
        "fraud[\"AgeOfVehicle\"] = pd.to_numeric(fraud[\"AgeOfVehicle\"])\n",
        "fraud.AgeOfVehicle.dtype"
      ],
      "metadata": {
        "id": "5Wy4UxVqLCsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2B2: FraudFound_P Data Type--Transformation to categorical**\n",
        "If we want to predict **FraudFound_P**, it needs to be categorical and not numeric (aka int64). This is why we need to transform this attribute from numeric to categorical."
      ],
      "metadata": {
        "id": "1oaPacDM6t67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud[\"FraudFound_P\"] = fraud[\"FraudFound_P\"].astype('object')\n",
        "print(\"Data Type is now\",fraud.FraudFound_P.dtype)"
      ],
      "metadata": {
        "id": "YLDQVNO57e4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2B2: DECISION NEEDED: PastNumberOfClaims**\n",
        "How to treat PastNumberOfClaims requires **decision-making**."
      ],
      "metadata": {
        "id": "TCwJCq8kBbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud.PastNumberOfClaims.dtype"
      ],
      "metadata": {
        "id": "oouCubOYCILW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud.PastNumberOfClaims.unique()"
      ],
      "metadata": {
        "id": "usVxB_4LCPDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THE DECISION**\n",
        "\n",
        "1. We are seeing that originally, PastNumberOfClaims is numeric.\n",
        "If we use it as a numeric output attribute in a DL model, we will need to use a regression setup: 1-node output layer with MSE as loss function and \"linear\" as activation function\n",
        "2. We are also seeing that PastNumberOfClaims has only 10 different values. So, it would be possible to use it as a categorical setup: 10-node output layer with categorical_crossentropy as loss function and \"softmax\" as activation function. To do this, however, we would have to convert PastNumberOfClaims into a categorical attribute (you saw above how to do this with FraudFound_P).\n",
        "\n",
        "<center>\n",
        "<img src = \"https://github.com/shstreuber/Data-Mining/blob/master/images/regorclass.JPG?raw=true\" height = 400>\n",
        "</center>\n",
        "\n",
        "What are you going to choose?\n"
      ],
      "metadata": {
        "id": "Sn8MCZOCCZiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 1: IF YOU DECIDE TO USE NUMBER 2 ABOVE: Transform PastNumberOfClaims from numeric to object**\n",
        "Use the line below to transform PastNumberOfClaims just like what you have seen above and remember to check whether your transformation was successful!\n",
        "\n",
        "To keep your options open, don't simply replace the value; instead, add add a new column in which to store the new datatype, like this:\n",
        "\n",
        "```\n",
        "# Creating a new attribute and populating it with the contents of the attribute that we want to transform.\n",
        "PastNumberOfClaims_cat=fraud.PastNumberOfClaims\n",
        "\n",
        "# Adding the new attribute to the dataframe\n",
        "fraud['PastNumberOfClaims_cat'] = PastNumberOfClaims_cat\n",
        "```\n",
        "\n",
        "Then you can use the example code from FraudFound_P to convert PastNumberOfClaims_cat to type object."
      ],
      "metadata": {
        "id": "p2rHvtNZ9RdL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ot8gl7V69olo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2B3: DECISION NEEDED: Determining the Data Frames**\n",
        "With the MNIST dataset, we saw that when processing even simple images, we found ourselves with 100,000 predictor attributes in the input layer. That is computationally problematic, especially with almost half of our 27 columns holding categorical attributes, which will need to be one-hot encoded to work in a Neural Network and inflate the input layer even more. With the usual [pay-as-you-go cloud processing plans](https://www.techtarget.com/searchstorage/definition/pay-as-you-go-cloud-computing-PAYG-cloud-computing), that can get expensive.\n",
        "\n",
        "So, we will need to make a few judgment calls and either rely on:\n",
        "* Common Sense: If we want to predict our target variable, which attributes are less relevant than others?\n",
        "* Mathematical methods to determine relevance of predictors to the target attribute: For example, correlation and chi square\n",
        "\n",
        "**DATA FRAMES AND TARGET VARIABLES**\n",
        "\n",
        "To simplify our work and the processing $$, we should select specific inputs for specific target variables and build smaller dataframes. Here are some options:\n",
        "1. car attributes to predict VehiclePrice aka replacement cost (fraud_1 dataset)\n",
        "2. driver attributes to predict PastNumberOfClaims--either as regression or classification (fraud_2 dataset)\n",
        "3. accident-related attributes to predict FraudFound_P (fraud_3 dataset)\n",
        "\n",
        "Other, not fraud-related, options could be:\n",
        "1. Recommend a base policy to sell based on driver attributes\n",
        "2. What other options can you come up with?\n",
        "\n",
        "I will show you an example of fraud_1."
      ],
      "metadata": {
        "id": "GuBrpU64NWAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Building data frame fraud_1**"
      ],
      "metadata": {
        "id": "9BpkNWjR_O9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Car attributes to predict vehicle price\n",
        "fraud_1 = fraud[['Make', 'VehicleCategory', 'AgeOfVehicle','NumberOfCars','VehiclePrice' ]]\n",
        "fraud_1.head()"
      ],
      "metadata": {
        "id": "e-iKroaBeis9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Ideas for data frame fraud_2**"
      ],
      "metadata": {
        "id": "2oRN6BpaZY0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_2 = fraud[['Sex','MaritalStatus','Age','Fault','DriverRating','PastNumberOfClaims']]\n",
        "fraud_2.head()"
      ],
      "metadata": {
        "id": "7Rs-mMovZdxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 2: What would you include in data frame fraud_3 with output FraudFound_P?**"
      ],
      "metadata": {
        "id": "KYfrl-pfaMN5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CM0BgcZaRwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_3.dtypes"
      ],
      "metadata": {
        "id": "UO8-bie6cQKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2B4: Preparing the Data for Use with Tensorflow**\n",
        "There are 4 steps we need to take to prepare the data to run with TensorFlow (before we even consider the architecture of the network):\n",
        "1. Encoding categorical variables\n",
        "2. Setting up training and test set\n",
        "3. Splitting features from labels (to build the input and output layers)\n",
        "4. Normalize all numeric features"
      ],
      "metadata": {
        "id": "IT4_LeEVLRXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We currently have 5 attributes in the fraud_1 dataset; how many are categorical and how many numeric?\n",
        "fraud_1.dtypes"
      ],
      "metadata": {
        "id": "OG-IcNOHgppl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1. One Hot Encoding**\n",
        "Remember that the input layer for a Neural Network requires numeric information only. We have two categorical variables in our reduced dataset: Make and VehicleCategory."
      ],
      "metadata": {
        "id": "VvD9DgMbMUre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using One-Hot Encoding with pd.getdummies\n",
        "fraud_1 = pd.get_dummies(fraud_1, columns=['Make','VehicleCategory'], prefix='', prefix_sep='')\n",
        "# This will give us a boolean output with True and False. In order to continue, we will need to convert the output to 1 (True) and 0 (False)\n",
        "\n",
        "# Here is the conversion to integers)\n",
        "fraud_1 = fraud_1.astype(int)\n",
        "\n",
        "fraud_1.head()"
      ],
      "metadata": {
        "id": "IdxEMA3gMPnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our three numeric variables appear in the first three columns of the dataset; our encoded variables appear to their right. AND you can see how they have been translated into ones and zeroes. That is what our Neural Network needs in order to process the data."
      ],
      "metadata": {
        "id": "zsY33CfVQ7sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2. Setting up training and test set**\n",
        "There are different ways to split the data into a training and test set. You can specify a split by line indexes, by percentages, or by number of rows. In our example, we will use percentages to split."
      ],
      "metadata": {
        "id": "4CQHuS7eS4Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = fraud_1.sample(frac=0.8, random_state=0) # training dataset is 80%, test dataset is 20%. Rows are picked by random sampling\n",
        "test_dataset = fraud_1.drop(train_dataset.index) # Dropping the index numbers because we want the test set to be autonomous"
      ],
      "metadata": {
        "id": "wEq2nULwTidX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.head()"
      ],
      "metadata": {
        "id": "4r3neAiqUKqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.head()"
      ],
      "metadata": {
        "id": "k-4GABlPUQXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvddecDbbF0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**3. Splitting Features from Labels**\n",
        "Separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict--in our case, we want to predict FraudFound_P."
      ],
      "metadata": {
        "id": "FWInxsjmUf_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "train_vals = train_features.pop('VehiclePrice')\n",
        "test_vals = test_features.pop('VehiclePrice')"
      ],
      "metadata": {
        "id": "DFdYUeiKUsBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features"
      ],
      "metadata": {
        "id": "vE2cQ__uU_eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 3: Check the shape of features and labels**\n",
        "In the code field below, check the shape of the training features and the training labels. Do the same with the test features and the test labels."
      ],
      "metadata": {
        "id": "fKOrLXk_WTgI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuyYjQwWWUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**4. Normalize ALL NUMERIC features**\n",
        "**Why do we need to normalize?**\n",
        "Remember when we hot-encoded the categorical variables? That's when we translated all categories into either 0 or 1. If one variable has a much larger scale than the others, it can dominate the learning process and overshadow the importance of other features. It's like studying for a test where one chapter is ten times longer than the others – you might end up focusing too much on that chapter and missing out on important information from the others.\n",
        "\n",
        "To level the playing field and ensure that all features contribute equally to the learning process, we normalize them. Normalization scales all features to a similar range, typically between 0 and 1 or with a mean of 0 and a standard deviation of 1. This way, each feature's contribution is balanced, and the neural network can learn more effectively from all the data.\n",
        "\n",
        "There are different ways of normalizing data. One way is to do the math manually as you have seen in the previous week's file. Another way is to use the [**preprocessing.Normalization layer**](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). This layer is a clean and simple way to build that preprocessing into your model. And YES--by creating the normalization layer, you effectively just started building your TensorFlow model:"
      ],
      "metadata": {
        "id": "cYC4r5XOWh8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalization(axis=-1)"
      ],
      "metadata": {
        "id": "UJVlh-qvYja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we apply the normalizer to the training features:"
      ],
      "metadata": {
        "id": "9kzAXNweYkR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer.adapt(np.array(train_features))"
      ],
      "metadata": {
        "id": "ptCRkiPCYmIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This calculates the mean and variance, and stores them in the layer."
      ],
      "metadata": {
        "id": "JafRBLVKYotG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(normalizer.mean.numpy())"
      ],
      "metadata": {
        "id": "QT5JAi0fYsBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the layer is called it returns the input data, with each feature independently normalized:"
      ],
      "metadata": {
        "id": "NDQ5DKruYwB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('Original data:', first)\n",
        "  print()\n",
        "  print('Normalized data:', normalizer(first).numpy())"
      ],
      "metadata": {
        "id": "AyTmtbZ2Ywpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 4: Are we ready to begin the model? Or do we need to take another step?**\n",
        "If so, describe the step below and then perform it."
      ],
      "metadata": {
        "id": "nJqi1ljgM7Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X19m31VNNMn9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmTgPTL9NM_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3A. Building the Model**\n",
        "There is always a specific process with which to build a TensorFlow model:\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/TF_Process2.png\" width=\"600\">\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "H0ASsd3aQ5F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 5: Which Loss Function do we need to use if our output variable is VehiclePrice?**\n",
        "Write your answer in the text field below:"
      ],
      "metadata": {
        "id": "BvPK_HQ4ZrWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double-click here to type in your answer."
      ],
      "metadata": {
        "id": "1ZejlDZNZ8eM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.1 Setting up a keras sequential model**\n",
        "We will build our network as follows:\n",
        "1. Use keras.Sequential\n",
        "2. Add our normalizer layer\n",
        "3. Add two hidden layers with 64 nodes each; we will use the [relu function](https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning) so that all positive values will remain positive but all negative values will become 0.\n",
        "4. For the output layer, we will need the \"linear\" function because, with regression, we have just one output (VehiclePrice). We use [the sigmoid and softmax functions ](https://towardsdatascience.com/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9) for output layers with binary or > 2 classes\n",
        "\n",
        "**How do we know the number and architecture of layers in the middle?**\n",
        "\n",
        "The short answer is: We don't. The longer answer is: We experiment until we get the best output the fastest. The even longer answer is: We can use various optimization strategies that can help us out somewhat. So, let's assume that trial and error has shown us that three layers is optimal. Furthermore, let's assume that we are going to build a Dense Network, aka a fully connected network structure, in which every node is connected with every node in the next layer."
      ],
      "metadata": {
        "id": "u8bZkWBcZ-CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the Keras model\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(64, input_dim=24, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "VCo56X5Oe-VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.2 Compiling the model**\n",
        "Now we can configure the training procedure using the Model.compile() method. The most important arguments to compile are the loss and the optimizer since these define what will be optimized and how (using the [optimizers.Adam](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.tensorflow.org%2Fapi_docs%2Fpython%2Ftf%2Fkeras%2Foptimizers%2FAdam)).\n",
        "\n",
        "**REMINDER**\n",
        "* Regression: MSE (mean square error) with MAE (mean absolute error) as metric\n",
        "* Binary: binary_crossentropy with accuracy as metric\n",
        "* Classification: categorical_crossentropy with accuracy as metric"
      ],
      "metadata": {
        "id": "AAqfRAzVhDhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='MSE', metrics=['MAE'])"
      ],
      "metadata": {
        "id": "BOcKJAgYhSbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sra36aI_QIeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.3 Training the model**\n",
        "Once the model is configured, we use Model.fit() to train it (give this about 1-2 minutes):"
      ],
      "metadata": {
        "id": "aUTuwvFnhnpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = model.fit(\n",
        "    train_features, train_vals,\n",
        "    epochs=10,\n",
        "    # suppress logging\n",
        "    verbose=1,\n",
        "    # Calculate validation results on 20% of the training data. Validation means that we test as we go, on a 20% subset of the training data\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "z3z_HrEmhzJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.4 Evaluating the model**\n",
        "We have trained our neural network and we can now evaluate the performance of the network on the test dataset. To evaluate your model on your training dataset, use the evaluate() function on your model and pass it the test data.\n",
        "\n",
        "This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n",
        "\n",
        "The evaluate() function will return a list with two values. The first will be the loss of the model on the dataset and the second will be the accuracy of the model on the dataset. We will be looking at the accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "vgmdUgMEpv0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the keras model\n",
        "model.evaluate(test_features, test_vals)"
      ],
      "metadata": {
        "id": "c89nN9zop_Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**SOLUTIONS**"
      ],
      "metadata": {
        "id": "geKebJDnPmDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 1: PastNumberOfClaims**\n",
        "\n",
        "Here is the solution to translating PastNumberOfClaims to categorical data type\n",
        "\n",
        "```\n",
        "fraud[\"PastNumberOfClaims\"] = fraud[\"PastNumberOfClaims\"].astype('object')\n",
        "print(\"Data Type is now\",fraud.PastNumberOfClaims.dtype)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qK3cpLRNSpos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 2: Building data frame for FraudFound_P**\n",
        "\n",
        "```\n",
        "fraud_3 = fraud[['AccidentArea','Fault','PoliceReportFiled','WitnessPresent','BasePolicy','FraudFound_P']]\n",
        "fraud_3.head()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JCuEBY6CTlOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 3: Check the shape of features and labels**\n",
        "\n",
        "Here is one example:\n",
        "```\n",
        "train_features.shape\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Mx8F843mT_M5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 4: Are we ready to begin the model? Or do we need to take another step?**\n",
        "\n",
        "Not ready yet! The preprocessing.Normalization layer normalizes only the input features, but not the output layer. We still need to normalize the target variable. Here is one way of how to do that (using [Prof. Glass' explanation](https://youtu.be/MgMIGLQeVu0?si=1NIaWF60ZbrUHh9l&t=480))\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "cmin = min(fraud_1['VehiclePrice'])\n",
        "cmax = max(fraud_1['VehiclePrice'])\n",
        "normalization_factor = cmax - cmin\n",
        "train_vals =  (train_vals - cmin/normalization_factor)\n",
        "test_vals =  (test_vals - cmin/normalization_factor)\n",
        "test_vals.head()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kwSeH-iNPr8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**EXERCISE 5: Which Loss Function do we need to use if our output variable is VehiclePrice?**\n",
        "\n",
        "* If regression, there are three options: You can use mean absolute error (MAE) or mean squared error (MSE) or the Huber Loss Function (huber_mse, huber_mae), which lies in the middle between MAE and MSE. This is described [here](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3).\n",
        "* If classification, use categorical_crossentropy"
      ],
      "metadata": {
        "id": "3-7nyaOeUn0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**FRIDAY LAB: Building the Fraud Prediction App**\n",
        "1. Determine which attributes to use for predicting FraudFound_P\n",
        "2. Build a dataframe with the attributes\n",
        "3. Preprocess the attributes so that they are usable with your model\n",
        "4. Build 80% training/ 20% test sets\n",
        "5. Remember to one-hot encode all necessary categorical attributes\n",
        "6. Build a model with 2 hidden layers and 64 nodes each to predict FraudFound_P\n",
        "5. Run the model with 100 and 1000 epochs\n",
        "6. Evaluate how the model performs."
      ],
      "metadata": {
        "id": "R9CY9xPzdNZm"
      }
    }
  ]
}