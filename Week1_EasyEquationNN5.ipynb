{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EasyEquationNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Portions Copyright 2019 The TensorFlow Authors.\n",
        "This notebook was edited from the TensorFlow Authors' original by Michael Glass and Jung Hee Kim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIAkIlfmCe1B"
      },
      "source": [
        "# Easy Neural Network to Compute a Linear Equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA93WUy1zzWf"
      },
      "source": [
        "This neural network will learn to compute a simple equation:\n",
        " y = 2*x + 1\n",
        "\n",
        "The neural network is trained by giving it examples of the input X value, where we already know the correct Y value. \n",
        "\n",
        "*   When an X value flows into the network, some predicted Y value comes out.\n",
        "*   The predicted Y is compared with the correct Y.\n",
        "* The difference (the error) is used to adjust the parameters in the network. The parameters are adjusted a little bit to nudge the network's output in the correct direction. \n",
        "* This process is repeated. \n",
        "Eventually the network's parameters are adjusted to the point where the output is (close to) correct for every X value in the training data.\n",
        "* Then we try some test data: we feed into the network some X values it has not seen before. \n",
        "\n",
        "Essentially we have produced a subroutine which computes the function, but we never explicitly wrote down the equation for the computer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzbtdRcZDO9B"
      },
      "source": [
        "## Imports\n",
        "Python imports:\n",
        "\n",
        "* Import `TensorFlow` and call it `tf` for ease of use.\n",
        "\n",
        "* Import `numpy`, for storing and manipulating arrays of numbers. (It is more flexible and efficient than using Python lists.)\n",
        "\n",
        "* The framework for defining a neural network as a set of Sequential layers is called ``keras``, so we import that too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9uIpOS2zx7k"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwJGmDrQ0EoB"
      },
      "source": [
        "## Define and Compile the Neural Network\n",
        "\n",
        "Next we will create the simplest possible neural network. It has 1 layer, and that layer has 1 neuron, and the input is just 1 value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQFAr_xo0M4T"
      },
      "source": [
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM_OT83IVG9E"
      },
      "source": [
        "Let's explain the above one line of Python code.\n",
        "* We construct a ``Sequential`` object, which contains a whole neural network. We put it in the variable ``model``.\n",
        "```\n",
        "   model = ...Sequential( ... )\n",
        "```\n",
        "* This neural network contains a Python list of layers:\n",
        "```\n",
        "   model = Sequential( [ layer1, layer2, ... ] )\n",
        "```\n",
        "* Here we have a single layer, which is a ``Dense`` object with only 1 neuron in the layer:\n",
        "```\n",
        "   model = Sequential( [ keras.layers.Dense(units=1...) ] )\n",
        "```\n",
        "* The input_shape is how we organized an array X values for one example case. If one example case is a single array of 13 numbers (13 numerical attributes) we would write [13]. But if each example has 50 pixel values organized as a 2-D 10 x 5 picture would write [10, 5]. In our case each example has 1 number. \n",
        "```\n",
        "   model = Sequential( [ Dense(units=1, input_shape=[1]) ] )\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sy9AenKnSsG"
      },
      "source": [
        "Keras neural network objects have a ``summary`` method, to show you what is inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsg4UazxngTj"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmUxF5R3n8WJ"
      },
      "source": [
        "The above should show that our neural network has one Dense layer. The output shape says this layer has one output, meaning it has one neuron. \n",
        "\n",
        "What is the meaning of the number of parameters? It shows us how many parameters are in each layer. For one neuron with one input:\n",
        "* The weight multiplier $w$, which is applied to the input value\n",
        "* The bias value $b$ for the neuron.\n",
        "\n",
        "By default the activation function is a simple linear equation. Since we have one $x$-value input this network computes: $y = wx + b$.\n",
        "\n",
        "However when we start, the network has no idea of the proper values of $w$ and $b$. Both these parameters are adjustable, meaning the neural nework will adjust their values as it learns.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjZjZ-c0Ok9"
      },
      "source": [
        "Now we compile the neural network. When we do so, we have to specify 2 functions, a loss and an optimizer.\n",
        "\n",
        "We know that in our function, the relationship between the numbers is y=2x-1. \n",
        "\n",
        "When the computer is trying to 'learn' that function, it makes a guess...maybe y=10x+10. The **loss** function measures the guessed answers against the known correct answers and measures how well or how badly it did.\n",
        "\n",
        "We will use a loss function MEAN SQUARED ERROR. Generally this looks at the square of each error ($\\Delta$ y), the difference between the expected output y-value and the correct output y-value.\n",
        "* When the output of one training case is near the goal (small error), the correction is small. \n",
        "* Traning cases which produce larger errors are generally more important.\n",
        "* The correction increases with the square of the error, so the correction will emphasize the training cases where the neural network is still not getting it right.\n",
        "\n",
        "It then uses the **optimizer** function to adjust the parameters in the neural network, nudging them in the direction of the correct output.\n",
        "\n",
        "The STOCASTIC GRADIENT DESCENT optimizer function. Gradient descent uses the derivative of the function y=f(z) which is inside each neuron.  Using the derivative we can estimate how much a small change in input will change the output. \n",
        "\n",
        "$\\Delta y$ = f'($z$) $\\Delta z$\n",
        "\n",
        " The loss function helped estimate how much output change $\\Delta$ y we need to achieve to get the error near zero. The derivative allows us to estimate the input change which would achieve that output change. The input value $x$ from the training data is multiplied by the weight $w$, so we adjust the weight. The neuron also utilzes the bias value $b$, another adjustable value. The job of the optimizer function is to guess small adjustments to the weight and bias values to correct the $\\Delta y$ error value. (Stochastic gradient descent is a modification of gradient descent.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8YQN1H41L-Y"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ActMmc4Pp2so"
      },
      "source": [
        "Remember that the `model` variable contains a neural network object.  The above line of Python code calls its `complile()` method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QyOUhFw1OUX"
      },
      "source": [
        "## Providing the Data\n",
        "\n",
        "Next we feed in some data. In this case we make 6 training cases, each has one X and one Y value. You can see that the relationship between these is that y=2x+1, so where x = -1, y=-1 etc.\n",
        "\n",
        "We will use numpy arrays. One array contains the X values, the another contains the Y values. Neural networks use floating point numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dxk4q-jzEy4"
      },
      "source": [
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-1.0,  1.0, 3.0, 5.0, 7.0, 9.0], dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5dF_edOxKR0"
      },
      "source": [
        "In the above code, the `np.array()` method converts a Python list into a Numpy array object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_YcWRElnM_b"
      },
      "source": [
        "# Training the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Jk4dG91dvD"
      },
      "source": [
        "The process of training the neural network, where it 'learns' the relationship between the Xs and Ys is in the ``model.fit()`` method. This is where it will go through the loop we spoke about above, making a guess, measuring how good or bad it is (aka the loss), using the opimizer to adjust the weights, make another guess etc. \n",
        "\n",
        "It will do it for the number of epochs you specify. Each epoch runs through all the training data once.\n",
        "\n",
        "When you run this code, you'll see the loss on the right hand side.\n",
        "\n",
        "First let us train for 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lff9c-EF5KrH"
      },
      "source": [
        "model.fit(xs, ys, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGD5Mb3b5R5M"
      },
      "source": [
        "The average error is likely pretty large, remember that it started with random numbers for the weight and bias. Let see how well the model computes our function $y=2x+1$ for an input value $x$=1.5. We can use the `model.predict()` method. \n",
        "\n",
        "The input is the same size and shape as we specified when we built the network: a 1-element array. Since a neural network could have multiple outputs, the output will also be in the form of an array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Yo-Ddl6H-Z"
      },
      "source": [
        "print(model.predict([1.5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C5134a_6ak_"
      },
      "source": [
        "Not very impressive. But we will train the model for 49 more epochs. Look at the loss value as it trains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpRrl7WK10Pq"
      },
      "source": [
        "model.fit(xs, ys, epochs=49)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaFIr71H2OZ-"
      },
      "source": [
        "Now the loss should be a much smaller number, you have a model that has been trained to learn the relationship between X and Y. You can use the `model.predict` method again to have it figure out the Y for a previously unknown X. Let us try this with several different input values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxNzL4lS2Gui"
      },
      "source": [
        "for i in [-2.0, 1.5, 6.0]:\n",
        "  print(i,'predicts',model.predict([i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vPBwq2py_KT"
      },
      "source": [
        "In the above code, `model.predict([...])` feeds to the network a list of the input numbers for one case. It returns the list of output numbers from the last layer.\n",
        "\n",
        "Again, our neural network has only one input number, so the list of one input number $i$ is `[i]`.\n",
        "\n",
        "How well is it working? \n",
        " \n",
        "It helps to know that the `loss` that is printed out is the average of the squares of all the errors. If an error was 0.2, the square is 0.04. So a `loss` of 0.04 means the errors were in the neighborhood of 0.2.\n",
        "\n",
        "The key here at the neural networks always work with approximations. The real-number (floating point number) inputs are multiplies by real-number weights, then added up, then put through some in the neuron. The error is used to adjust the weights by fractional amounts. So it is quite likely that this network will never be trained to produce exact values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZr0ZlYV27mw"
      },
      "source": [
        "\n",
        "We can do better, however. Let us try training the network for more epochs.\n",
        "\n",
        "Keep in mind that the `model` variable contains a neural network object which has been partially trained. We can use the `fit()` method to train it some more. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At1n0pkRzAPl"
      },
      "source": [
        "model.fit(xs, ys, epochs=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_OxMEsd2sx0"
      },
      "source": [
        "And we can test again. We will give it a few more values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XaSm1fy2u6Z"
      },
      "source": [
        "for i in [-5.0, -2.0, 1.5, 6.0, 10.0]:\n",
        "  print(i,'predicts',model.predict([i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLvuNgKq3ukN"
      },
      "source": [
        "Are these values good enough now? \n",
        "\n",
        "A more advanced topic is to add a function which is called after each epoch, which can evaluate the whether the network is good to stop training."
      ]
    }
  ]
}