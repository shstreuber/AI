{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5rc1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IsVCatp0Oswq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Portions Copyright 2019 The TensorFlow Authors.\n",
        "Modified by Jung Hee Kim and Michael Glass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnyTxjK_GbOD"
      },
      "source": [
        "# Beyond Hello World, A Computer Vision Example\n",
        "In the previous exercise you saw how to create a neural network that figured out the problem you were trying to solve. This gave an explicit example of learned behavior. Of course, in that instance, it was a bit of overkill because it would have been easier to write the function $y=2x+1$ directly, instead of bothering with using Machine Learning to learn the relationship between X and Y for a fixed set of values, and extending that for all values.\n",
        "\n",
        "But what about a scenario where writing rules like that is much more difficult -- for example a computer vision problem? Let's take a look at a scenario where we can recognize different items of clothing or a handwritten digit, trained from a dataset containing 10 different types of clothes or the 10 different digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H41FYgtlHPjW"
      },
      "source": [
        "## Start Coding\n",
        "\n",
        "Let's start with our import of TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3KzJyjv3rnA"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_n1U5do3u_F"
      },
      "source": [
        "The MNIST digit data and the Fashion MNIST data are available directly in the tf.keras datasets API.\n",
        "* MNIST is hand-written digits, scanned as 28x28 gray-scale images.\n",
        "* Fashion MNIST contains images of 10 different articles of apparel, also scanned as 28x28 gray-scale images.\n",
        "\n",
        "The following code cell shows how to load the two data sets, so you can experiment with either one.\n",
        "\n",
        "*We will use fashion for illustration. Later we can use digit images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmxkHFpt31bM"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist        # Clothing images\n",
        "#mnist = tf.keras.datasets.mnist               # Digit images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuoLQQBT4E-_"
      },
      "source": [
        "Calling load_data on this object will give you two sets of two lists, these will be the training and testing values for the graphics that contain the clothing items and their labels.\n",
        "\n",
        "* The input values were in arrays called ``_images``,\n",
        "* The corresponding correct output values are in arrays called ``_labels``\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdRgExe4TRB"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "print(\"Number of training cases=\", len(training_labels), \"  Number of test cases=\", len(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw395ROx4f5Q"
      },
      "source": [
        "What does these values look like? Let's print a training image, and a training label to see.\n",
        "Experiment with different indices in the array.\n",
        "\n",
        "The 10 different classes of apparel are described here: https://github.com/zalandoresearch/fashion-mnist#labels.\n",
        "\n",
        "We will illustrate with sneakers or digit 7, which have label=7 the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KpbC3LIIa8k"
      },
      "source": [
        "# This code will find the index numbers of the first 5 examples of any label\n",
        "target_label = 0   # Digit 7 (mnist digits) or Sneaker (fashion)\n",
        "cur_index = -1\n",
        "training_labels_list = training_labels.tolist()\n",
        "locations = []\n",
        "for i in range(5):\n",
        "  cur_index = training_labels_list.index(target_label, cur_index+1)\n",
        "  locations.append(cur_index)\n",
        "print(\"First 5 locations of label number\", target_label, \":\", locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPc9d3gJ3jWF"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(linewidth=120)\n",
        "\n",
        "# Pick an index number from the list of locations above\n",
        "indexNumber = 17\n",
        "\n",
        "print('Label=',training_labels[indexNumber])\n",
        "print(training_images[indexNumber])\n",
        "print(training_labels[:30])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbN0zsO7vsfY"
      },
      "source": [
        "We can use matplotlib to visualize the 28x28 image as gray-scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeAULTnLvjfm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(255-training_images[indexNumber], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no-tReQum_T7"
      },
      "source": [
        "Now try another index number for comparison between the above code cell and the below code cell.\n",
        "For example:\n",
        "* Fashion: try index numbers 6 vs. 14, or 0 vs. 42.\n",
        "* Digits: try index numbers 2 vs 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnvDblAnqzh6"
      },
      "source": [
        "indexNumber2 = 14 # In fashion 14 is another sneaker\n",
        "plt.imshow(255-training_images[indexNumber2], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cbrdH225_nH"
      },
      "source": [
        "You'll notice that all of the values in the number are between 0 and 255. If we are training a neural network, for various reasons it's easier if we treat all values as between 0.0 and 1.0, a process called '**normalizing**'. In Python-Numpy it's easy to normalize a numpy array by 'broadcasting', dividing an array by a single number will apply to the whole array.\n",
        "\n",
        "Remember that we loaded the data in two parts: training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRH19pWs6ZDn"
      },
      "source": [
        "# Normalize numbers to the range [0.0, 1.0]\n",
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "577OqK19rplC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIn7S9gf62ie"
      },
      "source": [
        "\n",
        "Now we build the neural network model. This one will have three layers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mAyndG3kVlK"
      },
      "source": [
        "model = tf.keras.models.Sequential([tf.keras.Input([28,28]),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    # tf.keras.layers.Dropout(rate=0.2),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7d1966yBuuQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lUcWaiX7MFj"
      },
      "source": [
        "**Sequential** neural network has a SEQUENCE of layers in the neural network\n",
        "\n",
        "**Flatten**: Input layer. Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set. The input is a 28 x 28 tensor.\n",
        "\n",
        "**Dense**: Middle layer. A layer of 128 neurons.\n",
        "* A *dense* layer means \"densely connected\" from the preceding layer. Every middle-layer neuron has a connection from every input-layer neuron.\n",
        "\n",
        "* **Relu** *rectified linear* activation function effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network. You saw relu in your EasyEquation neural network homework.\n",
        "\n",
        "**Dense**: Output layer. A layer of 10 neurons, densely connected from the middle layer.\n",
        "\n",
        "* **Softmax** activation effectively massages the 10 output values into the *appearance* of probabilities with the most likely answer boosted. The sum of all the values will be 1. So, for example\\\n",
        "``[0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05]`` is turned into\\\n",
        " ``[0,   0,   0,    0,   1,   0,   0,    0,    0]``  \n",
        "(not exactly 1.0, a little bit less, and not exactly 0.0, a little bit more).\n",
        "\n",
        "Note also that if there are two similar large values, meaning that your network thinks that both of those have similar high likelihood, sofmax will put both of them close to 0.5.\n",
        "\n",
        "Softmax is valuable for classification tasks, where the output of the neural network will tell us the **class** (category) of the data. In this lab we have 10 classes, numbered 0 to 9.  The softmax outputs are like this:\n",
        "* Label=0, output=``[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]``\n",
        "* Label=1, output=``[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]``\\\n",
        " ...\n",
        "* Label=9, output=``[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]``\n",
        "\n",
        "Note: the numbers won't be exactly 0.0 and 1.0, but close.\n",
        "\n",
        "Note also that softmax is a transfer function, it is the function within the neuron which transfers the weighted-sum-of-inputs into an output value. It is different from the other transfer functions in that the 10 output neurons are effectively cordinating with each other, deciding which is the biggest, adjusting their outputs so the sum is 1.0.\n",
        "\n",
        "**One-hot encoding** is the name we give to the above scheme for encoding the category labels. If you have N categories, you have N outputs -- where only 1 output is 1 and the others are 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVGjTJ1t-uTu"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOyTNY1XA8Fm"
      },
      "source": [
        "Summary:\n",
        "* Input layer has 784 neurons, it does no computation.\n",
        " *  It flattens the 28x28 2-D input tensor to 784 1-D vector, no adjustable parameters.\n",
        "* Middle layer has 128 neurons.\n",
        " * Each neuron receives 784 inputs from input layer, so each neuron has 784 weights.\n",
        " * Each neuron also has a bias input\n",
        " * So 785 adjustable parameters for each of 128 neurons = 100,480 adjustable parameters.\n",
        "* Output layer has 10 neurons.\n",
        " * Each receives 128 inputs (with weights) from middle layer\n",
        " * Plus a bias input\n",
        " * Total is 129 parameters for each of 10 neurons = 1,290 adjustable parameters\n",
        "* Grand total is 100,480 + 1,290 = 101,770"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRe9qAGxEoiz"
      },
      "source": [
        "The next thing to do, now the model is defined, is to actually build it. You do this by compiling it with an optimizer and loss function as before.\n",
        "Then you train it by calling the ``fit()`` method using the training data (the images and the labels).\n",
        "\n",
        "The *categorial cross-entropy* loss function is useful for classification tasks, when the neural network output is categories.\n",
        "\n",
        "The *Adam* optimizer is a variant of stochastic gradient descent, which we used\n",
        "in the first lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First compile the model, our chosen optimizer algorithm and loss metrics\n",
        "\n",
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "W5t5iTO4Qi47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWNzw8ElEbYN"
      },
      "source": [
        "# Now train\n",
        "model.fit(training_images, training_labels, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJMsvSB-1UY"
      },
      "source": [
        "Notice that this time we are reporting both the loss value and the accuracy. The accuracy is the fraction of cases that are correctly classified. Look at the accuracy value at the end of the final epoch. If it was approximately 0.89, that would tell you that your neural network is about 89% accurate in classifying the training data.\n",
        "\n",
        "But how would it work with unseen data? That's why we have the test data. We can call the ``evaluate()`` method, using the test images and the correct labels. It will report back the loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzlqsEzX9s5P"
      },
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tki-Aro_Uax"
      },
      "source": [
        "For me, that returned a accuracy of about 86% on the fashion.   As expected it\n",
        "probably would not do as well with *unseen* test data as it did with data it was trained on.\n",
        "\n",
        "To explore further, try the below exercises:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htldZNWcIPSN"
      },
      "source": [
        "# Exploration Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rquQqIx4AaGR"
      },
      "source": [
        "###Exercise 1:\n",
        "For this first exercise run the below code: It classifies each of the test images. Pick two different index numbers to see.\n",
        "The output, after you run it is a list of numbers. Why do you think this is?\n",
        "\n",
        "Hint: consider the output layer of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyEIki0z_hAD"
      },
      "source": [
        "classifications = model.predict(test_images)\n",
        "\n",
        "# Two example test cases\n",
        "testIndexNumber1=0\n",
        "testIndexNumber2=1\n",
        "\n",
        "print('test1 output=', classifications[testIndexNumber1])\n",
        "print('test2 output=', classifications[testIndexNumber2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdzqbQhRArzm"
      },
      "source": [
        "Now try running ``print(test_labels[indexNumber], test_labels[indexNumber2])``.\n",
        "Does that help you understand the output list?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnBGOrMiA1n5"
      },
      "source": [
        "\n",
        "#\n",
        "print(test_labels[testIndexNumber1])\n",
        "print(test_labels[testIndexNumber2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeB0SDlQQ5R8"
      },
      "source": [
        "plt.imshow(1.0-training_images[testIndexNumber2], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUs7eqr7uSvs"
      },
      "source": [
        "\n",
        "### What does an output list represent?\n",
        "\n",
        "\n",
        "1.   It's 10 random meaningless values\n",
        "2.   It's the first 10 classifications that the computer made\n",
        "3.   It's the probability that this item is each of the 10 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAbr92RTA67u"
      },
      "source": [
        "####Answer:\n",
        "The correct answer is (3)\n",
        "\n",
        "The output of the model is a list of 10 numbers. These numbers are a probability that the value being classified is the corresponding class.\n",
        "\n",
        "For the fashion data set, the labels are here: https://github.com/zalandoresearch/fashion-mnist#labels, i.e. the first value in the list is for class 0 (T-shirt/top), the next is a 1 (Trouser) etc.\n",
        "\n",
        "For the digits data set, the labels are the same as the digit. Class 0 means digit \"0\" was written, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgQSIfDSOWv6"
      },
      "source": [
        "##Exercise 2:\n",
        "Let's now look at the layers in your model. Experiment with different numbers of neurons in the middle layer, for example 1024 or 16 neurons. What different results do you get for loss, training time etc? Why do you think that's the case?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSZSwV5UObQP"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "#mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "modelB = tf.keras.models.Sequential([tf.keras.Input([28,28]),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(1000, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "modelB.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "modelB.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "print(\"\\nEvaluation on test data:\")\n",
        "modelB.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = modelB.predict(test_images)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOOEnHZFv5cS"
      },
      "source": [
        "###Question 1. Increase to 1024 Neurons -- What's the impact?\n",
        "\n",
        "1. Training takes longer, but is a lot more accurate\n",
        "2. Training takes longer, but only a little more accurate\n",
        "3. Training takes the same time, but is more accurate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U73MUP2lwrI2"
      },
      "source": [
        "####Answer\n",
        "The correct answer is by adding more neurons we have to do more calculations, slowing down the process, but in this case they have a good impact -- we do get more accurate. That doesn't mean it's always a case of 'more is better', you can hit the law of diminishing returns very quickly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtWxK16hQxLN"
      },
      "source": [
        "##Exercise 3:\n",
        "\n",
        "What would happen if you remove the Flatten() layer. Why do you think that's the case?\n",
        "\n",
        "You get an error about the shape of the data. It may seem vague right now, but it reinforces the rule of thumb that the first layer in your network should be the same shape as your data. Right now our data is 28x28 images, and 28 layers of 28 neurons would be infeasible, so it makes more sense to 'flatten' that 28,28 into a 784x1. Instead of wriitng all the code to handle that ourselves, we add the Flatten() layer at the begining, and when the arrays are loaded into the model later, they'll automatically be flattened for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExNxCwhcQ18S"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "#mnist = tf.keras.datasets.mnist\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "modelC = tf.keras.models.Sequential([#tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "modelC.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "modelC.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "modelC.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = modelC.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqoCR-ieSGDg"
      },
      "source": [
        "##Exercise 4:\n",
        "\n",
        "Consider the final (output) layers. Why are there 10 of them? What would happen if you had a different amount than 10? For example, try training the network with 5\n",
        "\n",
        "You get an error as soon as it finds an unexpected value. Another rule of thumb -- the number of neurons in the last layer should match the number of classes you are classifying for. In this case it's the digits 0-9, so there are 10 of them, hence you should have 10 neurons in your final layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMckVntcSPvo"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "modelC = tf.keras.models.Sequential([tf.keras.Input([28,28]),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])\n",
        "\n",
        "modelC.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "modelC.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "print(\"\\nEvaluating\")\n",
        "modelC.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = modelC.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0lF5MuvSuZF"
      },
      "source": [
        "##Exercise 5:\n",
        "\n",
        "Consider the effects of additional layers in the network. What will happen if you add another dense layer in the middle just before final layer with 10.\n",
        "\n",
        "Ans: There isn't a significant impact. For far more complex data extra layers are often necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1YPa6UhS8Es"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "#mnist = tf.keras.datasets.mnist\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "modelD = tf.keras.models.Sequential([tf.keras.Input([28,28]),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "modelD.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "modelD.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "print(\"\\nEvaluating\")\n",
        "modelD.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = modelD.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bql9fyaNUSFy"
      },
      "source": [
        "#Exercise 6:\n",
        "\n",
        "Consider the impact of training for more or less epochs.\n",
        "\n",
        "Try 15 epochs -- you'll probably get a model with a much better loss than the one with 5 epochs.\n",
        "Try 30 epochs -- you might see the loss value stops decreasing, and sometimes increases.\n",
        "There is also the problem of **overfitting** where the trained model has \"learned\" the specific data and doesn't work as well on data it hasn't seen yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE3esj9BURQe"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "#mnist = tf.keras.datasets.mnist\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "modelE = tf.keras.models.Sequential([tf.keras.Input([28,28]),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "modelE.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "modelE.fit(training_images, training_labels, epochs=15)\n",
        "\n",
        "print(\"\\nEvaluating\")\n",
        "modelE.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = modelE.predict(test_images)\n",
        "\n",
        "print(classifications[34])\n",
        "print(test_labels[34])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS3vVkOgCDGZ"
      },
      "source": [
        "#Exercise 7:\n",
        "\n",
        "Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1. What would be the impact of removing that? Here's the complete code to give it a try. Why do you think you get different results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDqNAqrpCNg0"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "#mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "#training_images=training_images/255.0\n",
        "#test_images=test_images/255.0\n",
        "modelF = tf.keras.models.Sequential([\n",
        "  tf.keras.Input([28,28]),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "modelF.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "modelF.fit(training_images, training_labels, epochs=5)\n",
        "print(\"\\nEvaluating\")\n",
        "modelF.evaluate(test_images, test_labels)\n",
        "classifications = modelF.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7W2PT66ZBHQ"
      },
      "source": [
        "#Exercise 8:\n",
        "\n",
        "Earlier when you trained for extra epochs you had an issue where your loss might change. It might have taken a bit of time for you to wait for the training to do that, and you might have thought 'wouldn't it be nice if I could stop the training when I reach a desired value?' -- i.e. 95% accuracy might be enough for you, and if you reach that after 3 epochs, why sit around waiting for it to finish a lot more epochs....So how would you fix that? Like any other program...you have callbacks! Let's see them in action..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkaEHHgqZbYv"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.85):\n",
        "      print(\"\\n\\nReached 85% accuracy so cancelling training!\\n\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images/255.0\n",
        "test_images=test_images/255.0\n",
        "modelG = tf.keras.models.Sequential([\n",
        "  tf.keras.Input([28,28]),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "modelG.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "modelG.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}